{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgY0e483bIOh",
    "outputId": "11358ecf-9a96-42ab-92cf-24cd4f82a2c9"
   },
   "outputs": [],
   "source": [
    "#!pip uninstall -q torchattacks\n",
    "#!pip install -q git+https://github.com/mohanrajroboticist/adversarial-attacks-pytorch \n",
    "#!pip install torchattacks -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1MnS2TEQVgO",
    "outputId": "5d504d52-7ff2-460e-bdea-4f96fe939678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import ResNet18_Weights\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from torch.quantization import quantize_fx\n",
    "import torchvision.transforms as transforms\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torchattacks\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "def load_test_data(data_dir):\n",
    "    #transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)   \n",
    "    return testset\n",
    "data_dir = '../../data'\n",
    "tes = load_test_data(data_dir)\n",
    "testloader = torch.utils.data.DataLoader(tes, batch_size=100, shuffle=False, num_workers=2)\n",
    "dataloader = {\"test\": testloader}\n",
    "dataiter = iter(dataloader['test'])\n",
    "img, lab = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "# if parameters['quantise'] == True:\n",
    "#     m = copy.deepcopy(model)\n",
    "#     m.to(\"cpu\")\n",
    "#     m.eval()\n",
    "#     qconfig_dict = {\"\": torch.quantization.get_default_qconfig(\"fbgemm\")}\n",
    "#     model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, img)\n",
    "#     with torch.inference_mode():\n",
    "#         for _ in range(10):\n",
    "#             img, lab = next(dataiter)\n",
    "#             model_prepared(img)\n",
    "#     q_model = quantize_fx.convert_fx(model_prepared)\n",
    "#     test_out = q_model(img)\n",
    "#     print(\"model quantisation success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers import get_model\n",
    "\n",
    "parameters = {  'num_classes': 10, \n",
    "                #'model_name':'LeNet',\n",
    "                'model_name':'Resnet18',\n",
    "                #'loss_function':'Evidential_MSE',\n",
    "                #'loss_function':'Evidential_LOG',\n",
    "                #'loss_function':'Evidential_DIGAMMA',\n",
    "                'loss_function': 'Crossentropy'\n",
    "             }\n",
    "\n",
    "\n",
    "models_path = '../../results/'\n",
    "\n",
    "\n",
    "model_path = str(models_path)+str(parameters['loss_function'])+'_'+str(parameters['model_name'])+'_model.pth'\n",
    "model = get_model(parameters['model_name'],num_classes=parameters['num_classes'],weights=None)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "model_path = str(models_path)+str(parameters['loss_function'])+'_'+str(parameters['model_name'])+'_quant_model.pth'\n",
    "net = get_model(parameters['model_name'],num_classes=parameters['num_classes'],weights=None)\n",
    "m = copy.deepcopy(net)\n",
    "m.to(\"cpu\")\n",
    "m.eval()\n",
    "qconfig_dict = {\"\": torch.quantization.get_default_qconfig(\"fbgemm\")}\n",
    "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, img)\n",
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        img, lab = next(dataiter)\n",
    "        model_prepared(img)\n",
    "q_model = quantize_fx.convert_fx(model_prepared)\n",
    "test_out = q_model(img)\n",
    "q_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PvlFUtKNhZ1p"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to('cpu')\n",
    "q_model.eval()\n",
    "q_model.to('cpu')\n",
    "\n",
    "\n",
    "inputs, labels = next(dataiter)\n",
    "inputs.to('cpu')\n",
    "labels.to('cpu')\n",
    "true_labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HB2ENWzpjSaH"
   },
   "outputs": [],
   "source": [
    "def run_test(net, images):\n",
    "    with torch.no_grad():\n",
    "        out = net(images)\n",
    "        _, preds = torch.max(out, 1)\n",
    "        predict_labels = np.array(preds)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predict_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mD-E5tBAj4OF",
    "outputId": "36c4b23c-862c-4132-bfa5-05d49ca513c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard WO ADV:  0.85\n",
      "Quantise WO ADV:  0.85\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input must have a range [0, 1] (max: 1.8768656253814697, min: -0.7145328521728516)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      7\u001b[0m atk \u001b[38;5;241m=\u001b[39m torchattacks\u001b[38;5;241m.\u001b[39mEADL1(model, torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), kappa\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m adv_images \u001b[38;5;241m=\u001b[39m \u001b[43matk\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m acc \u001b[38;5;241m=\u001b[39m run_test(model, adv_images)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStandard W  ADV: \u001b[39m\u001b[38;5;124m\"\u001b[39m, acc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchattacks/attack.py:471\u001b[0m, in \u001b[0;36mAttack.__call__\u001b[0;34m(self, images, labels, *args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m given_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_change_model_mode(given_training)\n\u001b[0;32m--> 471\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m adv_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(images, labels, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    473\u001b[0m adv_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_outputs(adv_images)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchattacks/attack.py:80\u001b[0m, in \u001b[0;36mAttack._check_inputs\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     78\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse_normalize(images)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmax(images) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mtol \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmin(images) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m\u001b[38;5;241m-\u001b[39mtol:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput must have a range [0, 1] (max: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, min: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     81\u001b[0m         torch\u001b[38;5;241m.\u001b[39mmax(images), torch\u001b[38;5;241m.\u001b[39mmin(images)))\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m images\n",
      "\u001b[0;31mValueError\u001b[0m: Input must have a range [0, 1] (max: 1.8768656253814697, min: -0.7145328521728516)"
     ]
    }
   ],
   "source": [
    "acc = run_test(model, inputs)\n",
    "print(\"Standard WO ADV: \", acc)\n",
    "acc = run_test(q_model, inputs)\n",
    "print(\"Quantise WO ADV: \", acc)\n",
    "\n",
    "atk = torchattacks.EADL1(model, torch.device('cpu'), kappa=8)\n",
    "adv_images = atk(inputs, labels)\n",
    "acc = run_test(model, adv_images)\n",
    "print(\"Standard W  ADV: \", acc)\n",
    "\n",
    "atk = torchattacks.EADL1(q_model, torch.device('cpu'), kappa=8)\n",
    "adv_images = atk(inputs, labels)\n",
    "acc = run_test(q_model, adv_images)\n",
    "print(\"Quantise W  ADV: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLOh4vPccJhq"
   },
   "outputs": [],
   "source": [
    "def imshow(img):    \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLHQsF1Tml2P"
   },
   "outputs": [],
   "source": [
    "imshow(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xm8WRmofmmn-"
   },
   "outputs": [],
   "source": [
    "imshow(adv_images[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
