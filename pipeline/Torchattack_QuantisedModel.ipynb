{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgY0e483bIOh",
    "outputId": "11358ecf-9a96-42ab-92cf-24cd4f82a2c9"
   },
   "outputs": [],
   "source": [
    "#!pip uninstall -q torchattacks\n",
    "#!pip install -q git+https://github.com/mohanrajroboticist/adversarial-attacks-pytorch \n",
    "#!pip install torchattacks -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1MnS2TEQVgO",
    "outputId": "5d504d52-7ff2-460e-bdea-4f96fe939678"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import ResNet18_Weights\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from torch.quantization import quantize_fx\n",
    "import torchvision.transforms as transforms\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torchattacks\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "def load_test_data(data_dir):\n",
    "    #transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)   \n",
    "    return testset\n",
    "data_dir = '../../data'\n",
    "tes = load_test_data(data_dir)\n",
    "testloader = torch.utils.data.DataLoader(tes, batch_size=100, shuffle=False, num_workers=2)\n",
    "dataloader = {\"test\": testloader}\n",
    "dataiter = iter(dataloader['test'])\n",
    "img, lab = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "# if parameters['quantise'] == True:\n",
    "#     m = copy.deepcopy(model)\n",
    "#     m.to(\"cpu\")\n",
    "#     m.eval()\n",
    "#     qconfig_dict = {\"\": torch.quantization.get_default_qconfig(\"fbgemm\")}\n",
    "#     model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, img)\n",
    "#     with torch.inference_mode():\n",
    "#         for _ in range(10):\n",
    "#             img, lab = next(dataiter)\n",
    "#             model_prepared(img)\n",
    "#     q_model = quantize_fx.convert_fx(model_prepared)\n",
    "#     test_out = q_model(img)\n",
    "#     print(\"model quantisation success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_model\n",
    "\n",
    "parameters = {  'num_classes': 10, \n",
    "                #'model_name':'LeNet',\n",
    "                'model_name':'Resnet18',\n",
    "                #'loss_function':'Evidential_MSE',\n",
    "                #'loss_function':'Evidential_LOG',\n",
    "                #'loss_function':'Evidential_DIGAMMA',\n",
    "                'loss_function': 'Crossentropy'\n",
    "             }\n",
    "\n",
    "\n",
    "models_path = '../../results/'\n",
    "\n",
    "\n",
    "model_path = str(models_path)+str(parameters['loss_function'])+'_'+str(parameters['model_name'])+'_model.pth'\n",
    "model = get_model(parameters['model_name'],num_classes=parameters['num_classes'],weights=None)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "model_path = str(models_path)+str(parameters['loss_function'])+'_'+str(parameters['model_name'])+'_quant_model.pth'\n",
    "net = get_model(parameters['model_name'],num_classes=parameters['num_classes'],weights=None)\n",
    "m = copy.deepcopy(net)\n",
    "m.to(\"cpu\")\n",
    "m.eval()\n",
    "qconfig_dict = {\"\": torch.quantization.get_default_qconfig(\"fbgemm\")}\n",
    "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, img)\n",
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        img, lab = next(dataiter)\n",
    "        model_prepared(img)\n",
    "q_model = quantize_fx.convert_fx(model_prepared)\n",
    "test_out = q_model(img)\n",
    "q_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PvlFUtKNhZ1p"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to('cpu')\n",
    "q_model.eval()\n",
    "q_model.to('cpu')\n",
    "\n",
    "\n",
    "inputs, labels = next(dataiter)\n",
    "inputs.to('cpu')\n",
    "labels.to('cpu')\n",
    "true_labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HB2ENWzpjSaH"
   },
   "outputs": [],
   "source": [
    "def run_test(net, images):\n",
    "    with torch.no_grad():\n",
    "        out = net(images)\n",
    "        _, preds = torch.max(out, 1)\n",
    "        predict_labels = np.array(preds)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predict_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mD-E5tBAj4OF",
    "outputId": "36c4b23c-862c-4132-bfa5-05d49ca513c6"
   },
   "outputs": [],
   "source": [
    "acc = run_test(model, inputs)\n",
    "print(\"Standard WO ADV: \", acc)\n",
    "acc = run_test(q_model, inputs)\n",
    "print(\"Quantise WO ADV: \", acc)\n",
    "\n",
    "atk = torchattacks.EADL1(model, torch.device('cpu'), kappa=8)\n",
    "adv_images = atk(inputs, labels)\n",
    "acc = run_test(model, adv_images)\n",
    "print(\"Standard W  ADV: \", acc)\n",
    "\n",
    "atk = torchattacks.EADL1(q_model, torch.device('cpu'), kappa=8)\n",
    "adv_images = atk(inputs, labels)\n",
    "acc = run_test(q_model, adv_images)\n",
    "print(\"Quantise W  ADV: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLOh4vPccJhq"
   },
   "outputs": [],
   "source": [
    "def imshow(img):    \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLHQsF1Tml2P"
   },
   "outputs": [],
   "source": [
    "imshow(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xm8WRmofmmn-"
   },
   "outputs": [],
   "source": [
    "imshow(adv_images[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
