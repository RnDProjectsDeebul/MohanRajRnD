{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising deep learning models\n",
    "Model optimization refers to the process of enhancing a model's performance or decreasing its complexity at the cost of some accuracy. Optimizing a model to reduce its complexity comes with numerous advantages, including the following:\n",
    "1. Reducing its size\n",
    "2. Decreasing latency\n",
    "3. Edge device compatibility\n",
    "\n",
    "## Optimisation types\n",
    "1. Pruning\n",
    "2. Clustering\n",
    "3. Quantisation\n",
    "\n",
    "## Pruning\n",
    "The process of pruning involves the elimination of parameters in a model that have a minimal effect on its predictions.\n",
    "\n",
    "## Clustering\n",
    "Clustering is a technique that involves organizing the weights of a model's layers into a fixed number of clusters, followed by the sharing of centroid values across the weights belonging to each of the individual clusters. As a result, the model's complexity is reduced since the number of unique weight values in the model is reduced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantisation\n",
    "Quantization is a technique that involves decreasing the precision of the numerical values used to represent a model's parameters, which are typically 32-bit floating point numbers by default.\n",
    "\n",
    "## Types of Quantisation\n",
    "1. Post Training Quantisation\n",
    "2. Quantisation Aware Training\n",
    "\n",
    "## Post Training Quantisation\n",
    "The process of decreasing the precision of a trained float model's parameters to a lighter version of the model is known as post-training quantisation.\n",
    "1. Static Quantisation\n",
    "2. Dynamic Quantisation\n",
    "\n",
    "### Static Quantisation (Post Training)\n",
    "To calculate the quantisation parameters, such as scale and zero point, a representative dataset is utilized on a trained floating-point model to determine the minimum and maximum values of weights and activations. These parameters are then applied to mitigate the potential loss resulting from quantisation.\n",
    "\n",
    "### Dynamic Quantisation (Post Training)\n",
    "In dynamic quantisation, the weights are quantised statically ahead of time after training and the activations are quantised dynamically during inference.\n",
    "\n",
    "The primary distinction between dynamic and static quantization lies in the method used to determine the scale and zero point of activations. In static quantization, these values are computed offline in advance, typically using a calibration dataset, and remain constant during every forward pass. Conversely, in dynamic quantization, the values are calculated on-the-fly during each forward pass, which results in more precise values but introduces additional computational overhead.\n",
    "\n",
    "## Quantisation Aware Training\n",
    "Quantization-aware training (QAT) involves incorporating simulated quantization into the training process of a neural network. This allows the network to learn optimal quantization parameters, in addition to its weights, using the training data. By simulating the effects of quantization during training, QAT can improve the accuracy and efficiency of the quantized model during inference. During QAT, floating-point arithmetic is used for forward and backward passes, while the quantization process is applied as a simulation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantisation parameters\n",
    "### Scale\n",
    "The scaling factor, known as the \"scale,\" is used to convert floating-point numbers to an integer representation by mapping the range of values that can be represented by a given number of bits in the integer representation.\n",
    "Example: A tensor containing float number ranging from -1.0 to 1.0 can be quantised with a scale factor of 0.1, which means that each integer in the 8-bit integer representation represents a range of 0.1 in the original floating-point range. Applying this scale factor, the mapping of floating-point values to integers would be as follows:\n",
    "\n",
    "-1.0 -> 0  <br>\n",
    "-0.5 -> 5  <br>\n",
    " 0.0 -> 10 <br>\n",
    " 0.5 -> 15 <br>\n",
    " 1.0 -> 20 <br>\n",
    "\n",
    " ### Zero point\n",
    " The zero point is the value that is subtracted from the floating-point value before quantization. It determines the offset of the range of values that can be represented by the integer representation.\n",
    " Example: A tensor containing float number ranging from -1.0 to 1.0 can be quantised with a scale factor of 0.1 and zero point of 128, which means that the float value is first subtracted by the zero point and then scale factor is applied.\n",
    "\n",
    "-1.0 -> 0   <br>\n",
    "-0.5 -> 64  <br>\n",
    " 0.0 -> 128 <br>\n",
    " 0.5 -> 192 <br>\n",
    " 1.0 -> 255 <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Quantisation Framework\n",
    "Eager mode is a beta feature and FX Graph mode is a new automated prototype feature. \n",
    "|                                  | Eager mode | FX Graph mode |\n",
    "|----------------------------------|------------|---------------|\n",
    "| Quant/DeQuant placement          | Manual     | Automatic     |\n",
    "| Operator Fusion                  | Manual     | Automatic     |\n",
    "| Quantising Functionals/Torch Ops | Manual     | Automatic     |\n",
    "\n",
    "* PyTorch supports only int8 quantisation\n",
    "* PyTorch supports post training static, dynamic and quantisation aware training\n",
    "* nn.Conv1d/2d/3d is not supported by dynamic quantisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
