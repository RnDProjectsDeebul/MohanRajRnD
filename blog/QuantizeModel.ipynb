{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "oF-wqwcX5IqR",
   "metadata": {
    "id": "oF-wqwcX5IqR"
   },
   "source": [
    "# Quantization of Pytorch Models\n",
    "\n",
    "## What is quantization\n",
    "* Quantization describes methods for carrying out calculations and storing tensors at smaller bit width than floating point precision. The default size of floating point numbers are 32 bits.\n",
    "* For instance, quantizing the deep learning model means, converting the 32-bit floating point numbers (of weights & activation outputs) to 8-bit integers.\n",
    "\n",
    "## Types of quantization \n",
    "* Post Training Quantization (PTQ)\n",
    "    1. Static\n",
    "    2. Dynamic/Weight only\n",
    "* Quantization Aware Training (QAT)\n",
    "    1. Static\n",
    "    \n",
    "| Pros | Cons |\n",
    "|---|---|\n",
    "| Model gets smaller | Potential for little degradation in accuracy | \n",
    "| Reduced memory usage during inferencing | |\n",
    "| Improves hardware accelerator latency | |\n",
    "| Reduces inference latency | |\n",
    "| Deployment on Edge AI devices with limited memory | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs0WI4gyhn7r",
   "metadata": {
    "id": "gs0WI4gyhn7r"
   },
   "source": [
    "## PyTorch Quantisation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "qNjGQxcbebSk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 950
    },
    "id": "qNjGQxcbebSk",
    "outputId": "3419d9e2-cfce-4d95-d968-3e5d393c182c"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"542pt\" height=\"696pt\" viewBox=\"0.00 0.00 541.50 696.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 692)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-692 537.5,-692 537.5,4 -4,4\"/>\n",
       "<!-- PyTorch\\nQuantisation Aware Training\\nTorch model -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>PyTorch\\nQuantisation Aware Training\\nTorch model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"173,-680.5 4,-680.5 0,-676.5 0,-627.5 169,-627.5 173,-631.5 173,-680.5\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"169,-676.5 0,-676.5 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"169,-676.5 169,-627.5 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"169,-676.5 173,-680.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"86.5\" y=\"-665.3\" font-family=\"Times,serif\" font-size=\"14.00\">PyTorch</text>\n",
       "<text text-anchor=\"middle\" x=\"86.5\" y=\"-650.3\" font-family=\"Times,serif\" font-size=\"14.00\">Quantisation Aware Training</text>\n",
       "<text text-anchor=\"middle\" x=\"86.5\" y=\"-635.3\" font-family=\"Times,serif\" font-size=\"14.00\">Torch model</text>\n",
       "</g>\n",
       "<!-- ONNX model -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>ONNX model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"319.5,-569 225.5,-569 225.5,-475 319.5,-475 319.5,-569\"/>\n",
       "<text text-anchor=\"middle\" x=\"272.5\" y=\"-518.3\" font-family=\"Times,serif\" font-size=\"14.00\">ONNX model</text>\n",
       "</g>\n",
       "<!-- PyTorch\\nQuantisation Aware Training\\nTorch model&#45;&gt;ONNX model -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>PyTorch\\nQuantisation Aware Training\\nTorch model-&gt;ONNX model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M123.25,-627.32C149.96,-608.65 186.58,-583.05 216.87,-561.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"219.03,-564.64 225.23,-556.04 215.02,-558.9 219.03,-564.64\"/>\n",
       "<text text-anchor=\"middle\" x=\"208\" y=\"-590.8\" font-family=\"Times,serif\" font-size=\"14.00\">Torch API</text>\n",
       "</g>\n",
       "<!-- PyTorch\\nPost Training Quantisation\\n(Dynamic/Weight only)\\nTorch model -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>PyTorch\\nPost Training Quantisation\\n(Dynamic/Weight only)\\nTorch model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"353.5,-688 195.5,-688 191.5,-684 191.5,-620 349.5,-620 353.5,-624 353.5,-688\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"349.5,-684 191.5,-684 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"349.5,-684 349.5,-620 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"349.5,-684 353.5,-688 \"/>\n",
       "<text text-anchor=\"middle\" x=\"272.5\" y=\"-672.8\" font-family=\"Times,serif\" font-size=\"14.00\">PyTorch</text>\n",
       "<text text-anchor=\"middle\" x=\"272.5\" y=\"-657.8\" font-family=\"Times,serif\" font-size=\"14.00\">Post Training Quantisation</text>\n",
       "<text text-anchor=\"middle\" x=\"272.5\" y=\"-642.8\" font-family=\"Times,serif\" font-size=\"14.00\">(Dynamic/Weight only)</text>\n",
       "<text text-anchor=\"middle\" x=\"272.5\" y=\"-627.8\" font-family=\"Times,serif\" font-size=\"14.00\">Torch model</text>\n",
       "</g>\n",
       "<!-- PyTorch\\nPost Training Quantisation\\n(Dynamic/Weight only)\\nTorch model&#45;&gt;ONNX model -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>PyTorch\\nPost Training Quantisation\\n(Dynamic/Weight only)\\nTorch model-&gt;ONNX model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M272.5,-619.86C272.5,-607.63 272.5,-593.4 272.5,-579.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"276,-579.32 272.5,-569.32 269,-579.32 276,-579.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"301\" y=\"-590.8\" font-family=\"Times,serif\" font-size=\"14.00\">Torch API</text>\n",
       "</g>\n",
       "<!-- PyTorch\\nPost Training Quantisation\\n(Static)\\nTorch model -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>PyTorch\\nPost Training Quantisation\\n(Static)\\nTorch model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"533.5,-688 375.5,-688 371.5,-684 371.5,-620 529.5,-620 533.5,-624 533.5,-688\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"529.5,-684 371.5,-684 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"529.5,-684 529.5,-620 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"529.5,-684 533.5,-688 \"/>\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-672.8\" font-family=\"Times,serif\" font-size=\"14.00\">PyTorch</text>\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-657.8\" font-family=\"Times,serif\" font-size=\"14.00\">Post Training Quantisation</text>\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-642.8\" font-family=\"Times,serif\" font-size=\"14.00\">(Static)</text>\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-627.8\" font-family=\"Times,serif\" font-size=\"14.00\">Torch model</text>\n",
       "</g>\n",
       "<!-- PyTorch\\nPost Training Quantisation\\n(Static)\\nTorch model&#45;&gt;ONNX model -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>PyTorch\\nPost Training Quantisation\\n(Static)\\nTorch model-&gt;ONNX model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M406.61,-619.86C382.66,-602.56 353.16,-581.25 327.93,-563.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"329.71,-560 319.56,-556.99 325.62,-565.68 329.71,-560\"/>\n",
       "<text text-anchor=\"middle\" x=\"408\" y=\"-590.8\" font-family=\"Times,serif\" font-size=\"14.00\">Torch API</text>\n",
       "</g>\n",
       "<!-- Blob model -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Blob model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"249,-424 168,-424 168,-343 249,-343 249,-424\"/>\n",
       "<text text-anchor=\"middle\" x=\"208.5\" y=\"-379.8\" font-family=\"Times,serif\" font-size=\"14.00\">Blob model</text>\n",
       "</g>\n",
       "<!-- ONNX model&#45;&gt;Blob model -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>ONNX model-&gt;Blob model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M225.21,-481.81C218.84,-474.27 213.23,-465.92 209.5,-457 206.57,-449.97 204.96,-442.22 204.22,-434.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"207.71,-434.17 203.71,-424.36 200.72,-434.52 207.71,-434.17\"/>\n",
       "<text text-anchor=\"middle\" x=\"262.5\" y=\"-445.8\" font-family=\"Times,serif\" font-size=\"14.00\">Blob converter API</text>\n",
       "</g>\n",
       "<!-- TF model -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>TF model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"373,-419 302,-419 302,-348 373,-348 373,-419\"/>\n",
       "<text text-anchor=\"middle\" x=\"337.5\" y=\"-379.8\" font-family=\"Times,serif\" font-size=\"14.00\">TF model</text>\n",
       "</g>\n",
       "<!-- ONNX model&#45;&gt;TF model -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>ONNX model-&gt;TF model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M305.86,-474.76C309.38,-468.93 312.69,-462.93 315.5,-457 319.69,-448.18 323.29,-438.38 326.31,-428.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"329.69,-429.8 329.23,-419.22 322.99,-427.78 329.69,-429.8\"/>\n",
       "<text text-anchor=\"middle\" x=\"353.5\" y=\"-445.8\" font-family=\"Times,serif\" font-size=\"14.00\">ONNX API</text>\n",
       "</g>\n",
       "<!-- TFLite model -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>TFLite model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"383.5,-292 291.5,-292 291.5,-200 383.5,-200 383.5,-292\"/>\n",
       "<text text-anchor=\"middle\" x=\"337.5\" y=\"-242.3\" font-family=\"Times,serif\" font-size=\"14.00\">TFLite model</text>\n",
       "</g>\n",
       "<!-- TF model&#45;&gt;TFLite model -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>TF model-&gt;TFLite model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M337.5,-347.96C337.5,-334.2 337.5,-318.01 337.5,-302.67\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"341,-302.23 337.5,-292.23 334,-302.23 341,-302.23\"/>\n",
       "<text text-anchor=\"middle\" x=\"382.5\" y=\"-313.8\" font-family=\"Times,serif\" font-size=\"14.00\">TensorFlow API</text>\n",
       "</g>\n",
       "<!-- EdgeTPU TFLite model -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>EdgeTPU TFLite model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"412,-149 263,-149 263,0 412,0 412,-149\"/>\n",
       "<text text-anchor=\"middle\" x=\"337.5\" y=\"-70.8\" font-family=\"Times,serif\" font-size=\"14.00\">EdgeTPU TFLite model</text>\n",
       "</g>\n",
       "<!-- TFLite model&#45;&gt;EdgeTPU TFLite model -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>TFLite model-&gt;EdgeTPU TFLite model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M337.5,-199.58C337.5,-187.11 337.5,-173.18 337.5,-159.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"341,-159.2 337.5,-149.2 334,-159.2 341,-159.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"382.5\" y=\"-170.8\" font-family=\"Times,serif\" font-size=\"14.00\">TensorFlow API</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from io import StringIO\n",
    "from IPython.display import SVG\n",
    "import pydot\n",
    "\n",
    "dot_graph1 = pydot.Dot(graph_type='digraph')\n",
    "\n",
    "sd_node = pydot.Node('PyTorch\\nQuantisation Aware Training\\nTorch model')\n",
    "sd_node.set_shape('box3d')\n",
    "dot_graph1.add_node(sd_node)\n",
    "\n",
    "bsd_node = pydot.Node('PyTorch\\nPost Training Quantisation\\n(Dynamic/Weight only)\\nTorch model')\n",
    "bsd_node.set_shape('box3d')\n",
    "dot_graph1.add_node(bsd_node)\n",
    "\n",
    "csd_node = pydot.Node('PyTorch\\nPost Training Quantisation\\n(Static)\\nTorch model')\n",
    "csd_node.set_shape('box3d')\n",
    "dot_graph1.add_node(csd_node)\n",
    "\n",
    "riq_node = pydot.Node('ONNX model')\n",
    "riq_node.set_shape('square')\n",
    "dot_graph1.add_node(riq_node)\n",
    "\n",
    "iedge = pydot.Edge(sd_node,riq_node)\n",
    "iedge.set_label('Torch API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "iedge = pydot.Edge(bsd_node,riq_node)\n",
    "iedge.set_label('Torch API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "iedge = pydot.Edge(csd_node,riq_node)\n",
    "iedge.set_label('Torch API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "asp_node = pydot.Node('Blob model')\n",
    "asp_node.set_shape('square')\n",
    "dot_graph1.add_node(asp_node)\n",
    "\n",
    "iedge = pydot.Edge(riq_node, asp_node)\n",
    "iedge.set_label('Blob converter API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "asp_node1 = pydot.Node('TF model')\n",
    "asp_node1.set_shape('square')\n",
    "dot_graph1.add_node(asp_node1)\n",
    "\n",
    "iedge = pydot.Edge(riq_node, asp_node1)\n",
    "iedge.set_label('ONNX API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "asp_node2 = pydot.Node('TFLite model')\n",
    "asp_node2.set_shape('square')\n",
    "dot_graph1.add_node(asp_node2)\n",
    "\n",
    "iedge = pydot.Edge(asp_node1, asp_node2)\n",
    "iedge.set_label('TensorFlow API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "asp_node3 = pydot.Node('EdgeTPU TFLite model')\n",
    "asp_node3.set_shape('square')\n",
    "dot_graph1.add_node(asp_node3)\n",
    "\n",
    "iedge = pydot.Edge(asp_node2, asp_node3)\n",
    "iedge.set_label('TensorFlow API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "dot_graph1.write_svg('big_data1.svg')\n",
    "dot_graph1.write_ps2('big_data1.ps2')\n",
    "SVG('big_data1.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vlfC6VUyhze5",
   "metadata": {
    "id": "vlfC6VUyhze5"
   },
   "source": [
    "## ONNX Quantisation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Ey0eaXzd5Gck",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ey0eaXzd5Gck",
    "outputId": "ef2a82fb-dc38-4857-b815-72f003fc0887"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"268pt\" height=\"826pt\" viewBox=\"0.00 0.00 267.50 826.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 822)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-822 263.5,-822 263.5,4 -4,4\"/>\n",
       "<!-- PyTorch\\nStandard Training\\nTorch model -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>PyTorch\\nStandard Training\\nTorch model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"161.5,-818 51.5,-818 47.5,-814 47.5,-765 157.5,-765 161.5,-769 161.5,-818\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"157.5,-814 47.5,-814 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"157.5,-814 157.5,-765 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"157.5,-814 161.5,-818 \"/>\n",
       "<text text-anchor=\"middle\" x=\"104.5\" y=\"-802.8\" font-family=\"Times,serif\" font-size=\"14.00\">PyTorch</text>\n",
       "<text text-anchor=\"middle\" x=\"104.5\" y=\"-787.8\" font-family=\"Times,serif\" font-size=\"14.00\">Standard Training</text>\n",
       "<text text-anchor=\"middle\" x=\"104.5\" y=\"-772.8\" font-family=\"Times,serif\" font-size=\"14.00\">Torch model</text>\n",
       "</g>\n",
       "<!-- ONNX model -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>ONNX model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"151.5,-714 57.5,-714 57.5,-620 151.5,-620 151.5,-714\"/>\n",
       "<text text-anchor=\"middle\" x=\"104.5\" y=\"-663.3\" font-family=\"Times,serif\" font-size=\"14.00\">ONNX model</text>\n",
       "</g>\n",
       "<!-- PyTorch\\nStandard Training\\nTorch model&#45;&gt;ONNX model -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>PyTorch\\nStandard Training\\nTorch model-&gt;ONNX model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M104.5,-764.82C104.5,-753.08 104.5,-738.67 104.5,-724.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"108,-724.43 104.5,-714.43 101,-724.43 108,-724.43\"/>\n",
       "<text text-anchor=\"middle\" x=\"133\" y=\"-735.8\" font-family=\"Times,serif\" font-size=\"14.00\">Torch API</text>\n",
       "</g>\n",
       "<!-- Quantised\\nONNX model -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Quantised\\nONNX model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"151.5,-569 57.5,-569 57.5,-475 151.5,-475 151.5,-569\"/>\n",
       "<text text-anchor=\"middle\" x=\"104.5\" y=\"-525.8\" font-family=\"Times,serif\" font-size=\"14.00\">Quantised</text>\n",
       "<text text-anchor=\"middle\" x=\"104.5\" y=\"-510.8\" font-family=\"Times,serif\" font-size=\"14.00\">ONNX model</text>\n",
       "</g>\n",
       "<!-- ONNX model&#45;&gt;Quantised\\nONNX model -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>ONNX model-&gt;Quantised\\nONNX model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M104.5,-619.97C104.5,-607.08 104.5,-592.93 104.5,-579.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"108,-579.44 104.5,-569.44 101,-579.44 108,-579.44\"/>\n",
       "<text text-anchor=\"middle\" x=\"136.5\" y=\"-590.8\" font-family=\"Times,serif\" font-size=\"14.00\">ONNX API</text>\n",
       "</g>\n",
       "<!-- Blob model -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Blob model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"81,-424 0,-424 0,-343 81,-343 81,-424\"/>\n",
       "<text text-anchor=\"middle\" x=\"40.5\" y=\"-379.8\" font-family=\"Times,serif\" font-size=\"14.00\">Blob model</text>\n",
       "</g>\n",
       "<!-- Quantised\\nONNX model&#45;&gt;Blob model -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Quantised\\nONNX model-&gt;Blob model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M57.21,-481.81C50.84,-474.27 45.23,-465.92 41.5,-457 38.57,-449.97 36.96,-442.22 36.22,-434.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"39.71,-434.17 35.71,-424.36 32.72,-434.52 39.71,-434.17\"/>\n",
       "<text text-anchor=\"middle\" x=\"94.5\" y=\"-445.8\" font-family=\"Times,serif\" font-size=\"14.00\">Blob converter API</text>\n",
       "</g>\n",
       "<!-- TF model -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>TF model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"205,-419 134,-419 134,-348 205,-348 205,-419\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.5\" y=\"-379.8\" font-family=\"Times,serif\" font-size=\"14.00\">TF model</text>\n",
       "</g>\n",
       "<!-- Quantised\\nONNX model&#45;&gt;TF model -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Quantised\\nONNX model-&gt;TF model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M137.86,-474.76C141.38,-468.93 144.69,-462.93 147.5,-457 151.69,-448.18 155.29,-438.38 158.31,-428.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"161.69,-429.8 161.23,-419.22 154.99,-427.78 161.69,-429.8\"/>\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-445.8\" font-family=\"Times,serif\" font-size=\"14.00\">ONNX API</text>\n",
       "</g>\n",
       "<!-- TFLite model -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>TFLite model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"215.5,-292 123.5,-292 123.5,-200 215.5,-200 215.5,-292\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.5\" y=\"-242.3\" font-family=\"Times,serif\" font-size=\"14.00\">TFLite model</text>\n",
       "</g>\n",
       "<!-- TF model&#45;&gt;TFLite model -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>TF model-&gt;TFLite model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M169.5,-347.96C169.5,-334.2 169.5,-318.01 169.5,-302.67\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"173,-302.23 169.5,-292.23 166,-302.23 173,-302.23\"/>\n",
       "<text text-anchor=\"middle\" x=\"214.5\" y=\"-313.8\" font-family=\"Times,serif\" font-size=\"14.00\">TensorFlow API</text>\n",
       "</g>\n",
       "<!-- EdgeTPU TFLite model -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>EdgeTPU TFLite model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"244,-149 95,-149 95,0 244,0 244,-149\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.5\" y=\"-70.8\" font-family=\"Times,serif\" font-size=\"14.00\">EdgeTPU TFLite model</text>\n",
       "</g>\n",
       "<!-- TFLite model&#45;&gt;EdgeTPU TFLite model -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>TFLite model-&gt;EdgeTPU TFLite model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M169.5,-199.58C169.5,-187.11 169.5,-173.18 169.5,-159.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"173,-159.2 169.5,-149.2 166,-159.2 173,-159.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"214.5\" y=\"-170.8\" font-family=\"Times,serif\" font-size=\"14.00\">TensorFlow API</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_graph = pydot.Dot(graph_type='digraph')\n",
    "\n",
    "sd_node = pydot.Node('PyTorch\\nStandard Training\\nTorch model')\n",
    "sd_node.set_shape('box3d')\n",
    "dot_graph.add_node(sd_node)\n",
    "\n",
    "riq_node = pydot.Node('ONNX model')\n",
    "riq_node.set_shape('square')\n",
    "dot_graph.add_node(riq_node)\n",
    "\n",
    "iedge = pydot.Edge(sd_node,riq_node)\n",
    "iedge.set_label('Torch API')\n",
    "dot_graph.add_edge(iedge)\n",
    "\n",
    "hadoop_node = pydot.Node('Quantised\\nONNX model')\n",
    "hadoop_node.set_shape('square')\n",
    "dot_graph.add_node(hadoop_node)\n",
    "\n",
    "iedge = pydot.Edge(riq_node,hadoop_node)\n",
    "iedge.set_label('ONNX API')\n",
    "dot_graph.add_edge(iedge)\n",
    "\n",
    "asp_node = pydot.Node('Blob model')\n",
    "asp_node.set_shape('square')\n",
    "dot_graph.add_node(asp_node)\n",
    "\n",
    "iedge = pydot.Edge(hadoop_node, asp_node)\n",
    "iedge.set_label('Blob converter API')\n",
    "dot_graph.add_edge(iedge)\n",
    "\n",
    "asp_node1 = pydot.Node('TF model')\n",
    "asp_node1.set_shape('square')\n",
    "dot_graph.add_node(asp_node1)\n",
    "\n",
    "iedge = pydot.Edge(hadoop_node, asp_node1)\n",
    "iedge.set_label('ONNX API')\n",
    "dot_graph.add_edge(iedge)\n",
    "\n",
    "asp_node2 = pydot.Node('TFLite model')\n",
    "asp_node2.set_shape('square')\n",
    "dot_graph.add_node(asp_node2)\n",
    "\n",
    "iedge = pydot.Edge(asp_node1, asp_node2)\n",
    "iedge.set_label('TensorFlow API')\n",
    "dot_graph.add_edge(iedge)\n",
    "\n",
    "asp_node3 = pydot.Node('EdgeTPU TFLite model')\n",
    "asp_node3.set_shape('square')\n",
    "dot_graph.add_node(asp_node3)\n",
    "\n",
    "iedge = pydot.Edge(asp_node2, asp_node3)\n",
    "iedge.set_label('TensorFlow API')\n",
    "dot_graph.add_edge(iedge)\n",
    "\n",
    "dot_graph.write_svg('big_data.svg')\n",
    "dot_graph.write_ps2('big_data.ps2')\n",
    "SVG('big_data.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cbd55c",
   "metadata": {
    "id": "f7cbd55c"
   },
   "source": [
    "## API\n",
    "1. PyTorch Quantization (QAT)\n",
    "    * https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html\n",
    "    * import torch\n",
    "    * from torchvision.models import MobileNetV2\n",
    "        * model = MobileNetV2()\n",
    "    * Fuse relu & Conv2d\n",
    "    * Insert Stubs to model \n",
    "        * model = nn.Sequential(torch.quantization.QuantStub(), model, torch.quantization.DeQuantStub())\n",
    "    * Prepare model\n",
    "        * m.train()\n",
    "        * backend = \"fbgemm\"\n",
    "        * model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "        * torch.quantization.prepare_qat(model, inplace=True)\n",
    "    * Run standard training loop\n",
    "    * Convert\n",
    "        * m.eval()\n",
    "        * model_quantized = torch.quantization.convert(model, inplace=True)\n",
    "        * torch.save(model_quantized, model_file_path)\n",
    "\n",
    "\n",
    "2. PyTorch -> ONNX\n",
    "    * https://pytorch.org/docs/stable/onnx.html\n",
    "    * import torch\n",
    "    * torch.onnx.export(model, sample_input, onnx_model_path, opset_version=12, input_names=['input'], output_names=['output'])\n",
    "\n",
    "\n",
    "3. ONNX Qunatization (Dynamic)\n",
    "    * https://onnxruntime.ai/docs/performance/quantization.html\n",
    "    * import onnx\n",
    "    * from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "    * quantized_model = quantize_dynamic(model_path, quantised_model_path)\n",
    "\n",
    "    \n",
    "4. ONNX -> Blob\n",
    "    * https://docs.luxonis.com/en/latest/pages/tutorials/creating-custom-nn-models/\n",
    "    * import blobconverter\n",
    "    * onnx_model = onnx.load(\"./results/networks/test1.onnx\")\n",
    "model_simpified, check = simplify(onnx_model)\n",
    "onnx.save(model_simpified, \"./results/networks/test_sim1.onnx\")\n",
    "    * blobconverter.from_onnx(model=onnx_model_path, data_type=\"FP16\", shaves=6, use_cache=False, output_dir=blob_model_path, optimizer_params=[])\n",
    "    \n",
    "    \n",
    "5. ONNX -> TF\n",
    "    * https://github.com/onnx/onnx-tensorflow/blob/main/example/onnx_to_tf.py\n",
    "    * import onnx\n",
    "    * from onnx_tf.backend import prepare\n",
    "    * onnx_model = onnx.load(onnx_model_path)\n",
    "    * tf_rep = prepare(onnx_model)\n",
    "    * tf_rep.export_graph(tf_model_path)\n",
    "    \n",
    "    \n",
    "6. TF -> TFLite\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter\n",
    "    * import tensorflow as tf\n",
    "    * converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
    "    * tflite_model = converter.convert()\n",
    "    * with open(tflite_model_path, 'wb') as f: f.write(tflite_model)\n",
    "    \n",
    "    \n",
    "7. TFLite -> EdgeTPU TFLite\n",
    "    * https://coral.ai/docs/edgetpu/compiler/\n",
    "    * curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "    * echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "    * sudo apt-get update\n",
    "    * sudo apt-get install edgetpu-compiler\n",
    "    * edgetpu_compiler [options] tflite_model_path\n",
    "    \n",
    "8. PyTorch Quantization (PTQ - Dynamic/Weight only)\n",
    "    * https://pytorch.org/blog/quantization-in-practice/\n",
    "9. PyTorch Quantization (PTQ - Static)\n",
    "    * https://pytorch.org/blog/quantization-in-practice/\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "MBrs6NA0uDlQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBrs6NA0uDlQ",
    "outputId": "5a1b987b-7b48-465c-a42b-ceff255aa86d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/tmp/MobileNet*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /tmp/MobileNet*\n",
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cCwPT5ZMRhHn",
   "metadata": {
    "id": "cCwPT5ZMRhHn"
   },
   "outputs": [],
   "source": [
    "torch_model_path = \"/tmp/MobileNetV2.pt\"\n",
    "torch_QAT_quant_path = \"/tmp/MobileNetV2_TorchQATQuant.pt\"\n",
    "onnx_model_path = \"/tmp/MobileNetV2.onnx\"\n",
    "onnx_quant_model_path = \"/tmp/MobileNetV2_OnnxQuant.onnx\"\n",
    "onnx_sim_model_path = \"/tmp/MobileNetV2_OnnxSim.onnx\"\n",
    "blob_model_path = \"/tmp/MobileNetV2.blob\"\n",
    "tf_model_path = \"/tmp/MobileNetV2.tf\"\n",
    "tflite_model_path = \"/tmp/MobileNetV2.tflite\"\n",
    "edgetpu_tflite_model_path = \"/tmp/MobileNetV2_edgetpu.tflite\"\n",
    "torch_PTQ_Weight_Eager_path = \"/tmp/MobileNet_V2_Torch_PTQ_Quant_W_EG.pt\"\n",
    "torch_PTQ_Weight_FX_path = \"/tmp/MobileNet_V2_Torch_PTQ_Quant_W_FX.pt\"\n",
    "torch_PTQ_Static_Eager_path = \"/tmp/MobileNet_V2_Torch_PTQ_Quant_S_EG.pt\"\n",
    "torch_PTQ_Static_FX_path = \"/tmp/MobileNet_V2_Torch_PTQ_Quant_S_FX.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1ed47",
   "metadata": {
    "id": "01e1ed47"
   },
   "source": [
    "# PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af1c3f8e",
   "metadata": {
    "id": "af1c3f8e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad4a1fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ad4a1fd",
    "outputId": "fbcbc6bf-f650-4059-91d3-c18449b5c480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV2()\n",
    "\n",
    "data_dir = os.path.abspath(\"./data\")\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.RandomErasing()])\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
    "train_sub_len = int(len(trainset) * 0.001)\n",
    "train_subset, val_subset = torch.utils.data.random_split(trainset, [train_sub_len, len(trainset) - train_sub_len])\n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "n_epochs = 3\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for images, labels in trainloader:\n",
    "        opt.zero_grad()\n",
    "        out = model(images)\n",
    "        loss = loss_fn(out, labels)\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3724fc3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3724fc3c",
    "outputId": "1031302f-5964-4bc1-d621-305c86905dc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 14M Feb 14 15:11 /tmp/MobileNetV2.pt\r\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, torch_model_path)\n",
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G6MK2m9LSG0j",
   "metadata": {
    "id": "G6MK2m9LSG0j"
   },
   "source": [
    "## 1. PyTorch Quantization (QAT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "kRFUX4vPSE9_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRFUX4vPSE9_",
    "outputId": "adb4ff26-199d-4fdf-f9e8-29a7628ba671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import MobileNetV2\n",
    "\n",
    "data_dir = os.path.abspath(\"./data\")\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.RandomErasing()])\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
    "train_sub_len = int(len(trainset) * 0.001)\n",
    "train_subset, val_subset = torch.utils.data.random_split(trainset, [train_sub_len, len(trainset) - train_sub_len])\n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "model = MobileNetV2()\n",
    "\n",
    "\"\"\"Fuse\"\"\"\n",
    "# pair_of_modules_to_fuze = []\n",
    "# for name, layer in model.named_modules():\n",
    "#     if isinstance(layer, torch.nn.Linear):\n",
    "#         pair_of_modules_to_fuze.append([name.split('.')[-1]])\n",
    "#     elif isinstance(layer, torch.nn.ReLU) and len(pair_of_modules_to_fuze) > 0:\n",
    "#         pair_of_modules_to_fuze[-1].append(name.split('.')[-1])\n",
    "# pair_of_modules_to_fuze = list(filter(lambda x: len(x) == 2, pair_of_modules_to_fuze))\n",
    "# torch.quantization.fuse_modules(model.modules(), pair_of_modules_to_fuze, inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"Insert stubs\"\"\"\n",
    "model = torch.nn.Sequential(torch.quantization.QuantStub(), \n",
    "                  model, \n",
    "                  torch.quantization.DeQuantStub())\n",
    "\n",
    "\n",
    "\"\"\"Prepare\"\"\"\n",
    "model.train()\n",
    "model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "torch.quantization.prepare_qat(model, inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"Training Loop\"\"\"\n",
    "n_epochs = 3\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        opt.zero_grad()\n",
    "        out = model(inputs)\n",
    "        loss = loss_fn(out, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "\n",
    "\"\"\"Convert\"\"\"\n",
    "model.eval()\n",
    "model_quantized = torch.quantization.convert(model, inplace=True)\n",
    "torch.save(model_quantized, torch_QAT_quant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "kskEfAfLthfJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kskEfAfLthfJ",
    "outputId": "bf493e31-37d0-4908-fcb9-e38a60b70bd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users  14M Feb 14 15:11 /tmp/MobileNetV2.pt\r\n",
      "-rw-r--r-- 1 jovyan users 4.1M Feb 14 15:12 /tmp/MobileNetV2_TorchQATQuant.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f81eda",
   "metadata": {
    "id": "71f81eda"
   },
   "source": [
    "# 2. PyTorch to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8cf4974",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8cf4974",
    "outputId": "904aef9d-b2fa-49f9-c778-f126b965f73b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users  14M Feb 14 15:12 /tmp/MobileNetV2.onnx\r\n",
      "-rw-r--r-- 1 jovyan users  14M Feb 14 15:11 /tmp/MobileNetV2.pt\r\n",
      "-rw-r--r-- 1 jovyan users 4.1M Feb 14 15:12 /tmp/MobileNetV2_TorchQATQuant.pt\r\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(torch_model_path)\n",
    "torch.onnx.export(model,               \n",
    "                  images,                         \n",
    "                  onnx_model_path,   \n",
    "                  export_params=True,             \n",
    "                  do_constant_folding=True,  \n",
    "                  input_names = ['input'],   \n",
    "                  output_names = ['output'], \n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},   \n",
    "                                'output' : {0 : 'batch_size'}})\n",
    "\n",
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a452551f",
   "metadata": {
    "id": "a452551f"
   },
   "source": [
    "# 3. ONNX Quantization (Dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "OjjoE_PDLyol",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OjjoE_PDLyol",
    "outputId": "593ce6f7-3104-446d-f636-c914ff725bf4"
   },
   "outputs": [],
   "source": [
    "!pip install onnx -q\n",
    "!pip install onnxruntime -q\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "VPzZYrACLekR",
   "metadata": {
    "id": "VPzZYrACLekR"
   },
   "outputs": [],
   "source": [
    "quantized_model = quantize_dynamic(onnx_model_path, onnx_quant_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ea74e",
   "metadata": {
    "id": "431ea74e"
   },
   "source": [
    "## 4. ONNX to Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d156d18d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d156d18d",
    "outputId": "6d93c5ae-a89d-4ff3-9428-036083e581e8"
   },
   "outputs": [],
   "source": [
    "!pip install onnxsim -q\n",
    "\n",
    "!pip install Flask==2.1.0 PyYAML==5.4.1 boto3==1.17.39 gunicorn==20.1.0 sentry-sdk -q\n",
    "!pip install blobconverter -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1fc9f61",
   "metadata": {
    "id": "d1fc9f61"
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxsim import simplify\n",
    "import blobconverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd9feee4",
   "metadata": {
    "id": "cd9feee4"
   },
   "outputs": [],
   "source": [
    "model_simpified, check = simplify(onnx_model_path)\n",
    "onnx.save(model_simpified, onnx_sim_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3699a52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3699a52",
    "outputId": "336c52e3-600f-4602-d39e-be2f49e68859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users  14M Feb 14 15:12 /tmp/MobileNetV2.onnx\r\n",
      "-rw-r--r-- 1 jovyan users 3.6M Feb 14 15:12 /tmp/MobileNetV2_OnnxQuant.onnx\r\n",
      "-rw-r--r-- 1 jovyan users  14M Feb 14 15:12 /tmp/MobileNetV2_OnnxSim.onnx\r\n",
      "-rw-r--r-- 1 jovyan users  14M Feb 14 15:11 /tmp/MobileNetV2.pt\r\n",
      "-rw-r--r-- 1 jovyan users 4.1M Feb 14 15:12 /tmp/MobileNetV2_TorchQATQuant.pt\r\n"
     ]
    }
   ],
   "source": [
    "# blobconverter.from_onnx(\n",
    "#     model=onnx_sim_model_path,\n",
    "#     data_type=\"FP16\",\n",
    "#     shaves=6,\n",
    "#     use_cache=False,\n",
    "#     output_dir=blob_model_path,\n",
    "#     optimizer_params=[])\n",
    "\n",
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eea7fe",
   "metadata": {
    "id": "d6eea7fe"
   },
   "source": [
    "## 5. ONNX to TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e4bb45e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e4bb45e",
    "outputId": "af25c0de-6622-4228-dec6-246cb4bdafc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-probability in /opt/conda/lib/python3.9/site-packages (0.19.0)\r\n",
      "Requirement already satisfied: gast>=0.3.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow-probability) (0.5.3)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-probability) (1.16.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow-probability) (2.1.0)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.9/site-packages (from tensorflow-probability) (5.1.1)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow-probability) (1.22.4)\r\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.9/site-packages (from tensorflow-probability) (0.1.8)\r\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from tensorflow-probability) (1.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx-tf -q\n",
    "!pip install tensorflow-probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37d37e07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37d37e07",
    "outputId": "d213d090-58c0-46e9-abc2-41446f33235a"
   },
   "outputs": [],
   "source": [
    "# from onnx_tf.backend import prepare\n",
    "# import tensorflow_probability\n",
    "# onnx_model = onnx.load(onnx_model_path)\n",
    "# tf_rep = prepare(onnx_model)\n",
    "# tf_rep.export_graph(tf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4X-s5c2MF2du",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4X-s5c2MF2du",
    "outputId": "44058334-4f3e-48b6-bf5f-d9597797b986"
   },
   "outputs": [],
   "source": [
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1zAZFSK0JphM",
   "metadata": {
    "id": "1zAZFSK0JphM"
   },
   "source": [
    "## 6. TF to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sTjoL9KaJ3M6",
   "metadata": {
    "id": "sTjoL9KaJ3M6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# def fake_dataset_generator(shape, n_iter):\n",
    "#     def dataset():\n",
    "#         for _ in range(n_iter):\n",
    "#             data = np.random.randn(*shape)\n",
    "#             data *= (1 / 255)\n",
    "#             batch = np.expand_dims(data, axis=0)\n",
    "#             yield [batch.astype(np.float32)]\n",
    "#     return dataset\n",
    "# datagen = fake_dataset_generator((192, 192, 3), 10)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
    "# converter.representative_dataset = datagen\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# converter.inference_input_type = tf.uint8\n",
    "# converter.inference_output_type = tf.uint8\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "with open(tflite_model_path, 'wb') as f: \n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oHE1_zxPKVof",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHE1_zxPKVof",
    "outputId": "f941f430-f373-4e89-ab43-aeed66756222"
   },
   "outputs": [],
   "source": [
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N4isbNDVLArM",
   "metadata": {
    "id": "N4isbNDVLArM"
   },
   "source": [
    "## 7. TFLite to EdgeTPU TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uHtyqR0EK_sA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHtyqR0EK_sA",
    "outputId": "4465299c-7d77-49ff-d2ea-397cb62cc769"
   },
   "outputs": [],
   "source": [
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install edgetpu-compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MtRoo-7WLPQV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MtRoo-7WLPQV",
    "outputId": "1cce9d9f-b3b8-4480-af4e-2bb0ac3279a1"
   },
   "outputs": [],
   "source": [
    "#https://github.com/google-coral/edgetpu/issues/453\n",
    "!edgetpu_compiler \"/tmp/MobileNetV2.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmB0g496MXLk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmB0g496MXLk",
    "outputId": "86d11be8-439e-44d1-dbca-d282d18d3c51"
   },
   "outputs": [],
   "source": [
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yvTE7GN3NNXy",
   "metadata": {
    "id": "yvTE7GN3NNXy"
   },
   "source": [
    "## PyTorch Quantization (PTQ - Dynamic/Weight only)\n",
    "\n",
    "https://pytorch.org/blog/quantization-in-practice/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ukzRPyNlfv9g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukzRPyNlfv9g",
    "outputId": "4591b584-5815-4271-b173-e314359dd38a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model = MobileNetV2()\n",
    "model.eval()\n",
    "\n",
    "## EAGER MODE\n",
    "from torch.quantization import quantize_dynamic\n",
    "model_quantized = quantize_dynamic(model=model, qconfig_spec={nn.LSTM, nn.Linear}, dtype=torch.qint8, inplace=False)\n",
    "torch.save(model_quantized, torch_PTQ_Weight_Eager_path)\n",
    "\n",
    "## FX MODE\n",
    "from torch.quantization import quantize_fx\n",
    "qconfig_dict = {\"\": torch.quantization.default_dynamic_qconfig} \n",
    "example_inputs = iter(trainloader)\n",
    "img, lab = next(example_inputs)\n",
    "model_prepared = quantize_fx.prepare_fx(model, qconfig_dict, img)\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "torch.save(model_quantized, torch_PTQ_Weight_FX_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yJLOrD6ykKLE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJLOrD6ykKLE",
    "outputId": "48f59f08-b584-4c0b-84b2-a5e6ae77c128"
   },
   "outputs": [],
   "source": [
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qgexvnSDf81J",
   "metadata": {
    "id": "qgexvnSDf81J"
   },
   "source": [
    "## PyTorch Quantization (PTQ - Static)\n",
    "\n",
    "https://pytorch.org/blog/quantization-in-practice/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6510cdd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6510cdd",
    "outputId": "fa7a319e-1c7b-4f4f-e69e-6020c9ad2a94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/ao/quantization/fx/prepare.py:1530: UserWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "# model = MobileNetV2(pretrained=True)\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "## EAGER MODE\n",
    "m = copy.deepcopy(model)\n",
    "m.eval()\n",
    "\n",
    "# torch.quantization.fuse_modules(m, ['0','1'], inplace=True) \n",
    "# torch.quantization.fuse_modules(m, ['2','3'], inplace=True) \n",
    "\n",
    "m = nn.Sequential(torch.quantization.QuantStub(), \n",
    "                  m, \n",
    "                  torch.quantization.DeQuantStub())\n",
    "\n",
    "m.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "torch.quantization.prepare(m, inplace=True)\n",
    "\n",
    "example_inputs = iter(trainloader)\n",
    "img, lab = next(example_inputs)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        m(img)\n",
    "model_quantized = torch.quantization.convert(m, inplace=True)\n",
    "torch.save(model_quantized, torch_PTQ_Static_Eager_path)\n",
    "\n",
    "\n",
    "## FX MODE\n",
    "from torch.quantization import quantize_fx\n",
    "m = copy.deepcopy(model)\n",
    "m.eval()\n",
    "qconfig_dict = {\"\": torch.quantization.get_default_qconfig(\"fbgemm\")}\n",
    "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, img)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        model_prepared(img)\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "torch.save(model_quantized, torch_PTQ_Static_FX_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "H6T8W_JAmhR-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H6T8W_JAmhR-",
    "outputId": "e1c33ec6-fb86-4efc-e27a-632c4a55a27d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users  14M Feb 14 15:12 /tmp/MobileNetV2.onnx\r\n",
      "-rw-r--r-- 1 jovyan users 3.6M Feb 14 15:12 /tmp/MobileNetV2_OnnxQuant.onnx\r\n",
      "-rw-r--r-- 1 jovyan users  14M Feb 14 15:12 /tmp/MobileNetV2_OnnxSim.onnx\r\n",
      "-rw-r--r-- 1 jovyan users  14M Feb 14 15:11 /tmp/MobileNetV2.pt\r\n",
      "-rw-r--r-- 1 jovyan users  12M Feb 14 15:13 /tmp/MobileNet_V2_Torch_PTQ_Quant_S_FX.pt\r\n",
      "-rw-r--r-- 1 jovyan users 4.1M Feb 14 15:12 /tmp/MobileNetV2_TorchQATQuant.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38725359",
   "metadata": {
    "id": "38725359"
   },
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.quantization.quantize_fx.prepare_fx.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd5d7e6",
   "metadata": {
    "id": "6dd5d7e6"
   },
   "source": [
    "## Test - QAT IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf547150",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf547150",
    "outputId": "7d5eccc2-cbd6-4c7f-9b9c-26f671d513d3"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "x, y = load_iris(return_X_y=True)\n",
    "train_X, test_X, train_y, test_y = train_test_split(x, y, test_size=0.8)\n",
    "train_X = Variable(torch.Tensor(train_X).float())\n",
    "test_X = Variable(torch.Tensor(test_X).float())\n",
    "train_y = Variable(torch.Tensor(train_y).long())\n",
    "test_y = Variable(torch.Tensor(test_y).long())\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.quant = QuantStub() \n",
    "        self.dequant = DeQuantStub() \n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.quant(X)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        X = self.fc3(X)\n",
    "        X = self.softmax(X)\n",
    "        X = self.dequant(X)\n",
    "        return X\n",
    "\n",
    "m = Net()\n",
    "\n",
    "m.train()\n",
    "backend = \"fbgemm\"\n",
    "m.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.quantization.prepare_qat(m, inplace=True)\n",
    "\n",
    "n_epochs = 10\n",
    "opt = torch.optim.SGD(m.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    opt.zero_grad()\n",
    "    out = m(train_X)\n",
    "    loss = loss_fn(out, train_y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "m.eval()\n",
    "model_quantized = torch.quantization.convert(m, inplace=True)\n",
    "torch.save(model_quantized, '/tmp/Test_QAT_iris.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Toj7rnNOhOOK",
   "metadata": {
    "id": "Toj7rnNOhOOK"
   },
   "source": [
    "## Test - Blob Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8c635",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33f8c635",
    "outputId": "900916f6-3b3d-4710-f360-f236761ff11b"
   },
   "outputs": [],
   "source": [
    "class CatImgs(nn.Module):\n",
    "    def forward(self, img1, img2, img3):\n",
    "        return torch.cat((img1, img2, img3), 3)\n",
    "\n",
    "\n",
    "X = torch.ones((1, 3, 300, 300), dtype=torch.float32)\n",
    "torch.onnx.export(\n",
    "    CatImgs(),\n",
    "    (X, X, X),\n",
    "    \"/tmp/Test_Blob_Onnx.onnx\",\n",
    "    opset_version=12,\n",
    "    do_constant_folding=True,\n",
    ")\n",
    "\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "\n",
    "onnx_model = onnx.load(\"/tmp/Test_Blob_Onnx.onnx\")\n",
    "model_simpified, check = simplify(onnx_model)\n",
    "onnx.save(model_simpified, \"/tmp/Test_Blob_OnnxSim.onnx\")\n",
    "\n",
    "import blobconverter\n",
    "\n",
    "blobconverter.from_onnx(\n",
    "    model=\"/tmp/Test_Blob_OnnxSim.onnx\",\n",
    "    output_dir=\"/tmp/Test_Blob.blob\",\n",
    "    data_type=\"FP16\",\n",
    "    shaves=6,\n",
    "    use_cache=False,\n",
    "    optimizer_params=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RDc0GKEdpgJb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDc0GKEdpgJb",
    "outputId": "fa212e43-90fd-4de7-b403-d42100d9c23b"
   },
   "outputs": [],
   "source": [
    "!ls -lh /tmp/MobileNet* & ls -lh /tmp/Test*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BRPYulJwgN2G",
   "metadata": {
    "id": "BRPYulJwgN2G"
   },
   "source": [
    "**TODO**\n",
    "1. Fix Blob converter for MobileNet \\\n",
    "2. Fix Compile TFLite to EdgeTPU TFLite \\"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
