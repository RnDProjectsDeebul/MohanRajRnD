{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "oF-wqwcX5IqR",
   "metadata": {
    "id": "oF-wqwcX5IqR"
   },
   "source": [
    "# Quantization of Pytorch Models\n",
    "\n",
    "## What is quantization\n",
    "* Quantization describes methods for carrying out calculations and storing tensors at smaller bit width than floating point precision. The default size of floating point numbers are 32 bits.\n",
    "* For instance, quantizing the deep learning model means, converting the 32-bit floating point numbers (of weights & activation outputs) to 8-bit integers.\n",
    "\n",
    "## Types of quantization \n",
    "* Post Training Quantization (PTQ)\n",
    "    1. Static\n",
    "    2. Dynamic/Weight only\n",
    "* Quantization Aware Training (QAT)\n",
    "    1. Static\n",
    "    \n",
    "| Pros | Cons |\n",
    "|---|---|\n",
    "| Model gets smaller | Potential for little degradation in accuracy | \n",
    "| Reduced memory usage during inferencing | |\n",
    "| Improves hardware accelerator latency | |\n",
    "| Reduces inference latency | |\n",
    "| Deployment on Edge AI devices with limited memory | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs0WI4gyhn7r",
   "metadata": {
    "id": "gs0WI4gyhn7r"
   },
   "source": [
    "## PyTorch Quantisation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "qNjGQxcbebSk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 950
    },
    "id": "qNjGQxcbebSk",
    "outputId": "3419d9e2-cfce-4d95-d968-3e5d393c182c"
   },
   "outputs": [
    {
     "ename": "ExpatError",
     "evalue": "syntax error: line 1, column 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExpatError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m dot_graph1\u001b[39m.\u001b[39mwrite_svg(\u001b[39m'\u001b[39m\u001b[39m/tmp/big_data1.svg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     71\u001b[0m dot_graph1\u001b[39m.\u001b[39mwrite_ps2(\u001b[39m'\u001b[39m\u001b[39m/tmp/big_data1.ps2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m SVG(\u001b[39m'\u001b[39;49m\u001b[39mbig_data1.svg\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rnd/lib/python3.10/site-packages/IPython/core/display.py:320\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m filename\n\u001b[1;32m    317\u001b[0m \u001b[39m# because of @data.setter methods in\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39m# subclasses ensure url and filename are set\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39m# before assigning to self.data\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata \u001b[39m=\u001b[39m data\n\u001b[1;32m    322\u001b[0m \u001b[39mif\u001b[39;00m metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m metadata\n",
      "File \u001b[0;32m~/anaconda3/envs/rnd/lib/python3.10/site-packages/IPython/core/display.py:491\u001b[0m, in \u001b[0;36mSVG.data\u001b[0;34m(self, svg)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[39m# parse into dom object\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdom\u001b[39;00m \u001b[39mimport\u001b[39;00m minidom\n\u001b[0;32m--> 491\u001b[0m x \u001b[39m=\u001b[39m minidom\u001b[39m.\u001b[39;49mparseString(svg)\n\u001b[1;32m    492\u001b[0m \u001b[39m# get svg tag (should be 1)\u001b[39;00m\n\u001b[1;32m    493\u001b[0m found_svg \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mgetElementsByTagName(\u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rnd/lib/python3.10/xml/dom/minidom.py:2000\u001b[0m, in \u001b[0;36mparseString\u001b[0;34m(string, parser)\u001b[0m\n\u001b[1;32m   1998\u001b[0m \u001b[39mif\u001b[39;00m parser \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1999\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mxml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdom\u001b[39;00m \u001b[39mimport\u001b[39;00m expatbuilder\n\u001b[0;32m-> 2000\u001b[0m     \u001b[39mreturn\u001b[39;00m expatbuilder\u001b[39m.\u001b[39;49mparseString(string)\n\u001b[1;32m   2001\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2002\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mxml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdom\u001b[39;00m \u001b[39mimport\u001b[39;00m pulldom\n",
      "File \u001b[0;32m~/anaconda3/envs/rnd/lib/python3.10/xml/dom/expatbuilder.py:925\u001b[0m, in \u001b[0;36mparseString\u001b[0;34m(string, namespaces)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     builder \u001b[39m=\u001b[39m ExpatBuilder()\n\u001b[0;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m builder\u001b[39m.\u001b[39;49mparseString(string)\n",
      "File \u001b[0;32m~/anaconda3/envs/rnd/lib/python3.10/xml/dom/expatbuilder.py:223\u001b[0m, in \u001b[0;36mExpatBuilder.parseString\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    221\u001b[0m parser \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetParser()\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     parser\u001b[39m.\u001b[39;49mParse(string, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    224\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_subset(string)\n\u001b[1;32m    225\u001b[0m \u001b[39mexcept\u001b[39;00m ParseEscape:\n",
      "\u001b[0;31mExpatError\u001b[0m: syntax error: line 1, column 0"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from io import StringIO\n",
    "from IPython.display import SVG\n",
    "import pydot\n",
    "\n",
    "dot_graph1 = pydot.Dot(graph_type='digraph', rankdir='LR')\n",
    "\n",
    "sd_node = pydot.Node('PyTorch\\nQuantisation Aware Training')\n",
    "sd_node.set_shape('box3d')\n",
    "dot_graph1.add_node(sd_node)\n",
    "\n",
    "bsd_node = pydot.Node('PyTorch\\nPost Training Quantisation\\n(Dynamic/Weight only)')\n",
    "bsd_node.set_shape('box3d')\n",
    "dot_graph1.add_node(bsd_node)\n",
    "\n",
    "csd_node = pydot.Node('PyTorch\\nPost Training Quantisation\\n(Static)')\n",
    "csd_node.set_shape('box3d')\n",
    "dot_graph1.add_node(csd_node)\n",
    "\n",
    "riq_node = pydot.Node('ONNX model')\n",
    "riq_node.set_shape('square')\n",
    "dot_graph1.add_node(riq_node)\n",
    "\n",
    "iedge = pydot.Edge(sd_node,riq_node)\n",
    "iedge.set_label('Torch API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "iedge = pydot.Edge(bsd_node,riq_node)\n",
    "iedge.set_label('Torch API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "iedge = pydot.Edge(csd_node,riq_node)\n",
    "iedge.set_label('Torch API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "asp_node = pydot.Node('Blob model')\n",
    "asp_node.set_shape('square')\n",
    "dot_graph1.add_node(asp_node)\n",
    "\n",
    "iedge = pydot.Edge(riq_node, asp_node)\n",
    "iedge.set_label('Blob converter API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "asp_node1 = pydot.Node('TensorFlow\\nmodel')\n",
    "asp_node1.set_shape('square')\n",
    "dot_graph1.add_node(asp_node1)\n",
    "\n",
    "iedge = pydot.Edge(riq_node, asp_node1)\n",
    "iedge.set_label('ONNX API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "asp_node2 = pydot.Node('TFLite model')\n",
    "asp_node2.set_shape('square')\n",
    "dot_graph1.add_node(asp_node2)\n",
    "\n",
    "iedge = pydot.Edge(asp_node1, asp_node2)\n",
    "iedge.set_label('TensorFlow API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "asp_node3 = pydot.Node('EdgeTPU\\nTFLite model')\n",
    "asp_node3.set_shape('square')\n",
    "dot_graph1.add_node(asp_node3)\n",
    "\n",
    "iedge = pydot.Edge(asp_node2, asp_node3)\n",
    "iedge.set_label('TensorFlow API')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "dot_graph1.write_svg('/tmp/big_data1.svg')\n",
    "dot_graph1.write_ps2('/tmp/big_data1.ps2')\n",
    "SVG('/tmp/big_data1.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vlfC6VUyhze5",
   "metadata": {
    "id": "vlfC6VUyhze5"
   },
   "source": [
    "## ONNX Quantisation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ey0eaXzd5Gck",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ey0eaXzd5Gck",
    "outputId": "ef2a82fb-dc38-4857-b815-72f003fc0887"
   },
   "outputs": [],
   "source": [
    "dot_graph = pydot.Dot(graph_type='digraph', rankdir='LR')\n",
    "\n",
    "sd_node = pydot.Node('PyTorch\\nStandard\\nTraining\\nTorch model')\n",
    "sd_node.set_shape('square')\n",
    "dot_graph.add_node(sd_node)\n",
    "\n",
    "riq_node = pydot.Node('ONNX model')\n",
    "riq_node.set_shape('square')\n",
    "dot_graph.add_node(riq_node)\n",
    "\n",
    "iedge = pydot.Edge(sd_node,riq_node)\n",
    "iedge.set_label('Torch API')\n",
    "dot_graph.add_edge(iedge)\n",
    "\n",
    "hadoop_node = pydot.Node('Quantised\\nONNX model')\n",
    "hadoop_node.set_shape('square')\n",
    "dot_graph.add_node(hadoop_node)\n",
    "\n",
    "iedge = pydot.Edge(riq_node,hadoop_node)\n",
    "iedge.set_label('ONNX API')\n",
    "dot_graph.add_edge(iedge)\n",
    "\n",
    "asp_node = pydot.Node('Blob model')\n",
    "asp_node.set_shape('square')\n",
    "dot_graph.add_node(asp_node)\n",
    "\n",
    "iedge = pydot.Edge(hadoop_node, asp_node)\n",
    "iedge.set_label('Blob converter API')\n",
    "dot_graph.add_edge(iedge)\n",
    "\n",
    "asp_node1 = pydot.Node('TensorFlow\\nmodel')\n",
    "asp_node1.set_shape('square')\n",
    "dot_graph.add_node(asp_node1)\n",
    "\n",
    "iedge = pydot.Edge(hadoop_node, asp_node1)\n",
    "iedge.set_label('ONNX API')\n",
    "dot_graph.add_edge(iedge)\n",
    "\n",
    "asp_node2 = pydot.Node('TFLite model')\n",
    "asp_node2.set_shape('square')\n",
    "dot_graph.add_node(asp_node2)\n",
    "\n",
    "iedge = pydot.Edge(asp_node1, asp_node2)\n",
    "iedge.set_label('TensorFlow API')\n",
    "dot_graph.add_edge(iedge)\n",
    "\n",
    "asp_node3 = pydot.Node('EdgeTPU\\nTFLite model')\n",
    "asp_node3.set_shape('square')\n",
    "dot_graph.add_node(asp_node3)\n",
    "\n",
    "iedge = pydot.Edge(asp_node2, asp_node3)\n",
    "iedge.set_label('TensorFlow API')\n",
    "dot_graph.add_edge(iedge)\n",
    "\n",
    "dot_graph.write_svg('/tmp/big_data.svg')\n",
    "dot_graph.write_ps2('/tmp/big_data.ps2')\n",
    "SVG('/tmp/big_data.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cbd55c",
   "metadata": {
    "id": "f7cbd55c"
   },
   "source": [
    "## API\n",
    "1. PyTorch Quantization (QAT)\n",
    "    * https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html\n",
    "    * import torch\n",
    "    * from torchvision.models import MobileNetV2\n",
    "        * model = MobileNetV2()\n",
    "    * Fuse relu & Conv2d\n",
    "    * Insert Stubs to model \n",
    "        * model = nn.Sequential(torch.quantization.QuantStub(), model, torch.quantization.DeQuantStub())\n",
    "    * Prepare model\n",
    "        * m.train()\n",
    "        * backend = \"fbgemm\"\n",
    "        * model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "        * torch.quantization.prepare_qat(model, inplace=True)\n",
    "    * Run standard training loop\n",
    "    * Convert\n",
    "        * m.eval()\n",
    "        * model_quantized = torch.quantization.convert(model, inplace=True)\n",
    "        * torch.save(model_quantized, model_file_path)\n",
    "\n",
    "\n",
    "2. PyTorch -> ONNX\n",
    "    * https://pytorch.org/docs/stable/onnx.html\n",
    "    * import torch\n",
    "    * torch.onnx.export(model, sample_input, onnx_model_path, opset_version=12, input_names=['input'], output_names=['output'])\n",
    "\n",
    "\n",
    "3. ONNX Qunatization (Dynamic)\n",
    "    * https://onnxruntime.ai/docs/performance/quantization.html\n",
    "    * import onnx\n",
    "    * from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "    * quantized_model = quantize_dynamic(model_path, quantised_model_path)\n",
    "\n",
    "    \n",
    "4. ONNX -> Blob\n",
    "    * https://docs.luxonis.com/en/latest/pages/tutorials/creating-custom-nn-models/\n",
    "    * import blobconverter\n",
    "    * onnx_model = onnx.load(\"./results/networks/test1.onnx\")\n",
    "model_simpified, check = simplify(onnx_model)\n",
    "onnx.save(model_simpified, \"./results/networks/test_sim1.onnx\")\n",
    "    * blobconverter.from_onnx(model=onnx_model_path, data_type=\"FP16\", shaves=6, use_cache=False, output_dir=blob_model_path, optimizer_params=[])\n",
    "    \n",
    "    \n",
    "5. ONNX -> TF\n",
    "    * https://github.com/onnx/onnx-tensorflow/blob/main/example/onnx_to_tf.py\n",
    "    * import onnx\n",
    "    * from onnx_tf.backend import prepare\n",
    "    * onnx_model = onnx.load(onnx_model_path)\n",
    "    * tf_rep = prepare(onnx_model)\n",
    "    * tf_rep.export_graph(tf_model_path)\n",
    "    \n",
    "    \n",
    "6. TF -> TFLite\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter\n",
    "    * import tensorflow as tf\n",
    "    * converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
    "    * tflite_model = converter.convert()\n",
    "    * with open(tflite_model_path, 'wb') as f: f.write(tflite_model)\n",
    "    \n",
    "    \n",
    "7. TFLite -> EdgeTPU TFLite\n",
    "    * https://coral.ai/docs/edgetpu/compiler/\n",
    "    * curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "    * echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "    * sudo apt-get update\n",
    "    * sudo apt-get install edgetpu-compiler\n",
    "    * edgetpu_compiler [options] tflite_model_path\n",
    "    \n",
    "8. PyTorch Quantization (PTQ - Dynamic/Weight only)\n",
    "    * https://pytorch.org/blog/quantization-in-practice/\n",
    "9. PyTorch Quantization (PTQ - Static)\n",
    "    * https://pytorch.org/blog/quantization-in-practice/\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MBrs6NA0uDlQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBrs6NA0uDlQ",
    "outputId": "5a1b987b-7b48-465c-a42b-ceff255aa86d"
   },
   "outputs": [],
   "source": [
    "!rm -rf /tmp/MobileNet*\n",
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cCwPT5ZMRhHn",
   "metadata": {
    "id": "cCwPT5ZMRhHn"
   },
   "outputs": [],
   "source": [
    "torch_model_path = \"/tmp/MobileNetV2.pt\"\n",
    "torch_QAT_quant_path = \"/tmp/MobileNetV2_TorchQATQuant.pt\"\n",
    "onnx_model_path = \"/tmp/MobileNetV2.onnx\"\n",
    "onnx_quant_model_path = \"/tmp/MobileNetV2_OnnxQuant.onnx\"\n",
    "onnx_sim_model_path = \"/tmp/MobileNetV2_OnnxSim.onnx\"\n",
    "blob_model_path = \"/tmp/MobileNetV2.blob\"\n",
    "tf_model_path = \"/tmp/MobileNetV2.tf\"\n",
    "tflite_model_path = \"/tmp/MobileNetV2.tflite\"\n",
    "edgetpu_tflite_model_path = \"/tmp/MobileNetV2_edgetpu.tflite\"\n",
    "torch_PTQ_Weight_Eager_path = \"/tmp/MobileNet_V2_Torch_PTQ_Quant_W_EG.pt\"\n",
    "torch_PTQ_Weight_FX_path = \"/tmp/MobileNet_V2_Torch_PTQ_Quant_W_FX.pt\"\n",
    "torch_PTQ_Static_Eager_path = \"/tmp/MobileNet_V2_Torch_PTQ_Quant_S_EG.pt\"\n",
    "torch_PTQ_Static_FX_path = \"/tmp/MobileNet_V2_Torch_PTQ_Quant_S_FX.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1ed47",
   "metadata": {
    "id": "01e1ed47"
   },
   "source": [
    "# PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c3f8e",
   "metadata": {
    "id": "af1c3f8e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4a1fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ad4a1fd",
    "outputId": "fbcbc6bf-f650-4059-91d3-c18449b5c480"
   },
   "outputs": [],
   "source": [
    "model = MobileNetV2()\n",
    "\n",
    "data_dir = os.path.abspath(\"../../data\")\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.RandomErasing()])\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
    "train_sub_len = int(len(trainset) * 0.001)\n",
    "train_subset, val_subset = torch.utils.data.random_split(trainset, [train_sub_len, len(trainset) - train_sub_len])\n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "n_epochs = 3\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for images, labels in trainloader:\n",
    "        opt.zero_grad()\n",
    "        out = model(images)\n",
    "        loss = loss_fn(out, labels)\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724fc3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3724fc3c",
    "outputId": "1031302f-5964-4bc1-d621-305c86905dc8"
   },
   "outputs": [],
   "source": [
    "torch.save(model, torch_model_path)\n",
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G6MK2m9LSG0j",
   "metadata": {
    "id": "G6MK2m9LSG0j"
   },
   "source": [
    "## 1. PyTorch Quantization (QAT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kRFUX4vPSE9_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRFUX4vPSE9_",
    "outputId": "adb4ff26-199d-4fdf-f9e8-29a7628ba671"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import MobileNetV2\n",
    "\n",
    "data_dir = os.path.abspath(\"../../data\")\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.RandomErasing()])\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
    "train_sub_len = int(len(trainset) * 0.001)\n",
    "train_subset, val_subset = torch.utils.data.random_split(trainset, [train_sub_len, len(trainset) - train_sub_len])\n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "model = MobileNetV2()\n",
    "\n",
    "\"\"\"Fuse\"\"\"\n",
    "# pair_of_modules_to_fuze = []\n",
    "# for name, layer in model.named_modules():\n",
    "#     if isinstance(layer, torch.nn.Linear):\n",
    "#         pair_of_modules_to_fuze.append([name.split('.')[-1]])\n",
    "#     elif isinstance(layer, torch.nn.ReLU) and len(pair_of_modules_to_fuze) > 0:\n",
    "#         pair_of_modules_to_fuze[-1].append(name.split('.')[-1])\n",
    "# pair_of_modules_to_fuze = list(filter(lambda x: len(x) == 2, pair_of_modules_to_fuze))\n",
    "# torch.quantization.fuse_modules(model.modules(), pair_of_modules_to_fuze, inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"Insert stubs\"\"\"\n",
    "model = torch.nn.Sequential(torch.quantization.QuantStub(), \n",
    "                  model, \n",
    "                  torch.quantization.DeQuantStub())\n",
    "\n",
    "\n",
    "\"\"\"Prepare\"\"\"\n",
    "model.train()\n",
    "model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "torch.quantization.prepare_qat(model, inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"Training Loop\"\"\"\n",
    "n_epochs = 3\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        opt.zero_grad()\n",
    "        out = model(inputs)\n",
    "        loss = loss_fn(out, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "\n",
    "\"\"\"Convert\"\"\"\n",
    "model.eval()\n",
    "model_quantized = torch.quantization.convert(model, inplace=True)\n",
    "torch.save(model_quantized, torch_QAT_quant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kskEfAfLthfJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kskEfAfLthfJ",
    "outputId": "bf493e31-37d0-4908-fcb9-e38a60b70bd5"
   },
   "outputs": [],
   "source": [
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f81eda",
   "metadata": {
    "id": "71f81eda"
   },
   "source": [
    "# 2. PyTorch to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf4974",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8cf4974",
    "outputId": "904aef9d-b2fa-49f9-c778-f126b965f73b"
   },
   "outputs": [],
   "source": [
    "model = torch.load(torch_model_path)\n",
    "torch.onnx.export(model,               \n",
    "                  images,                         \n",
    "                  onnx_model_path,   \n",
    "                  export_params=True,             \n",
    "                  do_constant_folding=True,  \n",
    "                  input_names = ['input'],   \n",
    "                  output_names = ['output'], \n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},   \n",
    "                                'output' : {0 : 'batch_size'}})\n",
    "\n",
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a452551f",
   "metadata": {
    "id": "a452551f"
   },
   "source": [
    "# 3. ONNX Quantization (Dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OjjoE_PDLyol",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OjjoE_PDLyol",
    "outputId": "593ce6f7-3104-446d-f636-c914ff725bf4"
   },
   "outputs": [],
   "source": [
    "!pip install onnx -q\n",
    "!pip install onnxruntime -q\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VPzZYrACLekR",
   "metadata": {
    "id": "VPzZYrACLekR"
   },
   "outputs": [],
   "source": [
    "quantized_model = quantize_dynamic(onnx_model_path, onnx_quant_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ea74e",
   "metadata": {
    "id": "431ea74e"
   },
   "source": [
    "## 4. ONNX to Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d156d18d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d156d18d",
    "outputId": "6d93c5ae-a89d-4ff3-9428-036083e581e8"
   },
   "outputs": [],
   "source": [
    "!pip install onnxsim -q\n",
    "\n",
    "!pip install Flask==2.1.0 PyYAML==5.4.1 boto3==1.17.39 gunicorn==20.1.0 sentry-sdk -q\n",
    "!pip install blobconverter -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc9f61",
   "metadata": {
    "id": "d1fc9f61"
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxsim import simplify\n",
    "import blobconverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9feee4",
   "metadata": {
    "id": "cd9feee4"
   },
   "outputs": [],
   "source": [
    "# model_simpified, check = simplify(onnx_model_path)\n",
    "# onnx.save(model_simpified, onnx_sim_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3699a52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3699a52",
    "outputId": "336c52e3-600f-4602-d39e-be2f49e68859"
   },
   "outputs": [],
   "source": [
    "# blobconverter.from_onnx(\n",
    "#     model=onnx_sim_model_path,\n",
    "#     data_type=\"FP16\",\n",
    "#     shaves=6,\n",
    "#     use_cache=False,\n",
    "#     output_dir=blob_model_path,\n",
    "#     optimizer_params=[])\n",
    "\n",
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eea7fe",
   "metadata": {
    "id": "d6eea7fe"
   },
   "source": [
    "## 5. ONNX to TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4bb45e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e4bb45e",
    "outputId": "af25c0de-6622-4228-dec6-246cb4bdafc4"
   },
   "outputs": [],
   "source": [
    "!pip install onnx-tf -q\n",
    "!pip install tensorflow-probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d37e07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37d37e07",
    "outputId": "d213d090-58c0-46e9-abc2-41446f33235a"
   },
   "outputs": [],
   "source": [
    "# from onnx_tf.backend import prepare\n",
    "# import tensorflow_probability\n",
    "# onnx_model = onnx.load(onnx_model_path)\n",
    "# tf_rep = prepare(onnx_model)\n",
    "# tf_rep.export_graph(tf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4X-s5c2MF2du",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4X-s5c2MF2du",
    "outputId": "44058334-4f3e-48b6-bf5f-d9597797b986"
   },
   "outputs": [],
   "source": [
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1zAZFSK0JphM",
   "metadata": {
    "id": "1zAZFSK0JphM"
   },
   "source": [
    "## 6. TF to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sTjoL9KaJ3M6",
   "metadata": {
    "id": "sTjoL9KaJ3M6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# def fake_dataset_generator(shape, n_iter):\n",
    "#     def dataset():\n",
    "#         for _ in range(n_iter):\n",
    "#             data = np.random.randn(*shape)\n",
    "#             data *= (1 / 255)\n",
    "#             batch = np.expand_dims(data, axis=0)\n",
    "#             yield [batch.astype(np.float32)]\n",
    "#     return dataset\n",
    "# datagen = fake_dataset_generator((192, 192, 3), 10)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
    "# converter.representative_dataset = datagen\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# converter.inference_input_type = tf.uint8\n",
    "# converter.inference_output_type = tf.uint8\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "with open(tflite_model_path, 'wb') as f: \n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oHE1_zxPKVof",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHE1_zxPKVof",
    "outputId": "f941f430-f373-4e89-ab43-aeed66756222"
   },
   "outputs": [],
   "source": [
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N4isbNDVLArM",
   "metadata": {
    "id": "N4isbNDVLArM"
   },
   "source": [
    "## 7. TFLite to EdgeTPU TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uHtyqR0EK_sA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHtyqR0EK_sA",
    "outputId": "4465299c-7d77-49ff-d2ea-397cb62cc769"
   },
   "outputs": [],
   "source": [
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install edgetpu-compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MtRoo-7WLPQV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MtRoo-7WLPQV",
    "outputId": "1cce9d9f-b3b8-4480-af4e-2bb0ac3279a1"
   },
   "outputs": [],
   "source": [
    "#https://github.com/google-coral/edgetpu/issues/453\n",
    "!edgetpu_compiler \"/tmp/MobileNetV2.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmB0g496MXLk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmB0g496MXLk",
    "outputId": "86d11be8-439e-44d1-dbca-d282d18d3c51"
   },
   "outputs": [],
   "source": [
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yvTE7GN3NNXy",
   "metadata": {
    "id": "yvTE7GN3NNXy"
   },
   "source": [
    "## PyTorch Quantization (PTQ - Dynamic/Weight only)\n",
    "\n",
    "https://pytorch.org/blog/quantization-in-practice/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ukzRPyNlfv9g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukzRPyNlfv9g",
    "outputId": "4591b584-5815-4271-b173-e314359dd38a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model = MobileNetV2()\n",
    "model.eval()\n",
    "\n",
    "## EAGER MODE\n",
    "from torch.quantization import quantize_dynamic\n",
    "model_quantized = quantize_dynamic(model=model, qconfig_spec={nn.LSTM, nn.Linear}, dtype=torch.qint8, inplace=False)\n",
    "torch.save(model_quantized, torch_PTQ_Weight_Eager_path)\n",
    "\n",
    "## FX MODE\n",
    "from torch.quantization import quantize_fx\n",
    "qconfig_dict = {\"\": torch.quantization.default_dynamic_qconfig} \n",
    "example_inputs = iter(trainloader)\n",
    "img, lab = next(example_inputs)\n",
    "model_prepared = quantize_fx.prepare_fx(model, qconfig_dict, img)\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "torch.save(model_quantized, torch_PTQ_Weight_FX_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yJLOrD6ykKLE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJLOrD6ykKLE",
    "outputId": "48f59f08-b584-4c0b-84b2-a5e6ae77c128"
   },
   "outputs": [],
   "source": [
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qgexvnSDf81J",
   "metadata": {
    "id": "qgexvnSDf81J"
   },
   "source": [
    "## PyTorch Quantization (PTQ - Static)\n",
    "\n",
    "https://pytorch.org/blog/quantization-in-practice/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6510cdd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6510cdd",
    "outputId": "fa7a319e-1c7b-4f4f-e69e-6020c9ad2a94"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "# model = MobileNetV2(pretrained=True)\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "## EAGER MODE\n",
    "m = copy.deepcopy(model)\n",
    "m.eval()\n",
    "\n",
    "# torch.quantization.fuse_modules(m, ['0','1'], inplace=True) \n",
    "# torch.quantization.fuse_modules(m, ['2','3'], inplace=True) \n",
    "\n",
    "m = nn.Sequential(torch.quantization.QuantStub(), \n",
    "                  m, \n",
    "                  torch.quantization.DeQuantStub())\n",
    "\n",
    "m.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "torch.quantization.prepare(m, inplace=True)\n",
    "\n",
    "example_inputs = iter(trainloader)\n",
    "img, lab = next(example_inputs)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        m(img)\n",
    "model_quantized = torch.quantization.convert(m, inplace=True)\n",
    "torch.save(model_quantized, torch_PTQ_Static_Eager_path)\n",
    "\n",
    "\n",
    "## FX MODE\n",
    "from torch.quantization import quantize_fx\n",
    "m = copy.deepcopy(model)\n",
    "m.eval()\n",
    "qconfig_dict = {\"\": torch.quantization.get_default_qconfig(\"fbgemm\")}\n",
    "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, img)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        model_prepared(img)\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "torch.save(model_quantized, torch_PTQ_Static_FX_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H6T8W_JAmhR-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H6T8W_JAmhR-",
    "outputId": "e1c33ec6-fb86-4efc-e27a-632c4a55a27d"
   },
   "outputs": [],
   "source": [
    "!ls -lh /tmp/MobileNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38725359",
   "metadata": {
    "id": "38725359"
   },
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.quantization.quantize_fx.prepare_fx.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd5d7e6",
   "metadata": {
    "id": "6dd5d7e6"
   },
   "source": [
    "## Test - QAT IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf547150",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf547150",
    "outputId": "7d5eccc2-cbd6-4c7f-9b9c-26f671d513d3"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "x, y = load_iris(return_X_y=True)\n",
    "train_X, test_X, train_y, test_y = train_test_split(x, y, test_size=0.8)\n",
    "train_X = Variable(torch.Tensor(train_X).float())\n",
    "test_X = Variable(torch.Tensor(test_X).float())\n",
    "train_y = Variable(torch.Tensor(train_y).long())\n",
    "test_y = Variable(torch.Tensor(test_y).long())\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.quant = QuantStub() \n",
    "        self.dequant = DeQuantStub() \n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.quant(X)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        X = self.fc3(X)\n",
    "        X = self.softmax(X)\n",
    "        X = self.dequant(X)\n",
    "        return X\n",
    "\n",
    "m = Net()\n",
    "\n",
    "m.train()\n",
    "backend = \"fbgemm\"\n",
    "m.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.quantization.prepare_qat(m, inplace=True)\n",
    "\n",
    "n_epochs = 10\n",
    "opt = torch.optim.SGD(m.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    opt.zero_grad()\n",
    "    out = m(train_X)\n",
    "    loss = loss_fn(out, train_y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "m.eval()\n",
    "model_quantized = torch.quantization.convert(m, inplace=True)\n",
    "torch.save(model_quantized, '/tmp/Test_QAT_iris.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Toj7rnNOhOOK",
   "metadata": {
    "id": "Toj7rnNOhOOK"
   },
   "source": [
    "## Test - Blob Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8c635",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33f8c635",
    "outputId": "900916f6-3b3d-4710-f360-f236761ff11b"
   },
   "outputs": [],
   "source": [
    "class CatImgs(nn.Module):\n",
    "    def forward(self, img1, img2, img3):\n",
    "        return torch.cat((img1, img2, img3), 3)\n",
    "\n",
    "\n",
    "X = torch.ones((1, 3, 300, 300), dtype=torch.float32)\n",
    "torch.onnx.export(\n",
    "    CatImgs(),\n",
    "    (X, X, X),\n",
    "    \"/tmp/Test_Blob_Onnx.onnx\",\n",
    "    opset_version=12,\n",
    "    do_constant_folding=True,\n",
    ")\n",
    "\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "\n",
    "onnx_model = onnx.load(\"/tmp/Test_Blob_Onnx.onnx\")\n",
    "model_simpified, check = simplify(onnx_model)\n",
    "onnx.save(model_simpified, \"/tmp/Test_Blob_OnnxSim.onnx\")\n",
    "\n",
    "import blobconverter\n",
    "\n",
    "blobconverter.from_onnx(\n",
    "    model=\"/tmp/Test_Blob_OnnxSim.onnx\",\n",
    "    output_dir=\"/tmp/Test_Blob.blob\",\n",
    "    data_type=\"FP16\",\n",
    "    shaves=6,\n",
    "    use_cache=False,\n",
    "    optimizer_params=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RDc0GKEdpgJb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDc0GKEdpgJb",
    "outputId": "fa212e43-90fd-4de7-b403-d42100d9c23b"
   },
   "outputs": [],
   "source": [
    "!ls -lh /tmp/MobileNet* & ls -lh /tmp/Test*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BRPYulJwgN2G",
   "metadata": {
    "id": "BRPYulJwgN2G"
   },
   "source": [
    "**TODO**\n",
    "1. Fix Blob converter for MobileNet \\\n",
    "2. Fix Compile TFLite to EdgeTPU TFLite \\"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f7e8fdd6c7089b66554fb6a2fb9ba4ef5dc66a5cacc103223d0f5e702d5f7a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
