{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287a0668",
   "metadata": {},
   "source": [
    "Evidential Deep Learning to Quantify Classification Uncertainty <br>\n",
    "Murat Sensoy, Lance Kaplan, Melih Kandemir <br>\n",
    "https://arxiv.org/abs/1806.01768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19839ff5",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Deep Learning models lack transparency in decision-making process. This leads to concerns about robustness and reliability in safety critical applications. A new approach called evidential deep learning has emerged to quantify uncertainty of a deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266a2aa",
   "metadata": {},
   "source": [
    "## NN with cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7536527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"497pt\" height=\"715pt\" viewBox=\"0.00 0.00 497.00 715.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 711)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-711 493,-711 493,4 -4,4\"/>\n",
       "<!-- Standard NN -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Standard NN</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"256,-707 172,-707 168,-703 168,-671 252,-671 256,-675 256,-707\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"252,-703 168,-703 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"252,-703 252,-671 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"252,-703 256,-707 \"/>\n",
       "<text text-anchor=\"middle\" x=\"212\" y=\"-685.3\" font-family=\"Times,serif\" font-size=\"14.00\">Standard NN</text>\n",
       "</g>\n",
       "<!-- Softmax \\n (Cross entropy loss) -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Softmax \\n (Cross entropy loss)</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"148,-581 18,-581 18,-451 148,-451 148,-581\"/>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-519.8\" font-family=\"Times,serif\" font-size=\"14.00\">Softmax </text>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-504.8\" font-family=\"Times,serif\" font-size=\"14.00\"> (Cross entropy loss)</text>\n",
       "</g>\n",
       "<!-- Standard NN&#45;&gt;Softmax \\n (Cross entropy loss) -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Standard NN-&gt;Softmax \\n (Cross entropy loss)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M184.51,-670.87C177.35,-665.66 169.97,-659.56 164,-653 147,-634.34 131.77,-611.57 119.23,-590.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122.2,-588.17 114.21,-581.21 116.12,-591.63 122.2,-588.17\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.5\" y=\"-641.8\" font-family=\"Times,serif\" font-size=\"14.00\">logits </text>\n",
       "<text text-anchor=\"middle\" x=\"190.5\" y=\"-626.8\" font-family=\"Times,serif\" font-size=\"14.00\"> [2, -1, 4]</text>\n",
       "</g>\n",
       "<!-- Predicted class = Max(logits) -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Predicted class = Max(logits)</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"344,-605 166,-605 166,-427 344,-427 344,-605\"/>\n",
       "<text text-anchor=\"middle\" x=\"255\" y=\"-512.3\" font-family=\"Times,serif\" font-size=\"14.00\">Predicted class = Max(logits)</text>\n",
       "</g>\n",
       "<!-- Standard NN&#45;&gt;Predicted class = Max(logits) -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Standard NN-&gt;Predicted class = Max(logits)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M216.28,-670.98C219.73,-657.28 224.87,-636.81 230.3,-615.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"233.76,-615.82 232.8,-605.27 226.97,-614.11 233.76,-615.82\"/>\n",
       "<text text-anchor=\"middle\" x=\"253.5\" y=\"-641.8\" font-family=\"Times,serif\" font-size=\"14.00\">logits </text>\n",
       "<text text-anchor=\"middle\" x=\"253.5\" y=\"-626.8\" font-family=\"Times,serif\" font-size=\"14.00\"> [2, -1, 4]</text>\n",
       "</g>\n",
       "<!-- NLL(Cross entropy loss) \\n \\n CE = &#45;∑_i(y_i * log(p_i)) \\n y_i = one hot label \\n p_i = class probabilities -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>NLL(Cross entropy loss) \\n \\n CE = -∑_i(y_i * log(p_i)) \\n y_i = one hot label \\n p_i = class probabilities</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"166,-361 0,-361 0,-195 166,-195 166,-361\"/>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-304.3\" font-family=\"Times,serif\" font-size=\"14.00\">NLL(Cross entropy loss) </text>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-289.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-274.3\" font-family=\"Times,serif\" font-size=\"14.00\"> CE = -∑_i(y_i * log(p_i)) </text>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-259.3\" font-family=\"Times,serif\" font-size=\"14.00\"> y_i = one hot label </text>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-244.3\" font-family=\"Times,serif\" font-size=\"14.00\"> p_i = class probabilities</text>\n",
       "</g>\n",
       "<!-- Softmax \\n (Cross entropy loss)&#45;&gt;NLL(Cross entropy loss) \\n \\n CE = &#45;∑_i(y_i * log(p_i)) \\n y_i = one hot label \\n p_i = class probabilities -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Softmax \\n (Cross entropy loss)-&gt;NLL(Cross entropy loss) \\n \\n CE = -∑_i(y_i * log(p_i)) \\n y_i = one hot label \\n p_i = class probabilities</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M83,-450.82C83,-426.38 83,-398 83,-371.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"86.5,-371.37 83,-361.37 79.5,-371.37 86.5,-371.37\"/>\n",
       "<text text-anchor=\"middle\" x=\"135.5\" y=\"-397.8\" font-family=\"Times,serif\" font-size=\"14.00\">Class probabilities </text>\n",
       "<text text-anchor=\"middle\" x=\"135.5\" y=\"-382.8\" font-family=\"Times,serif\" font-size=\"14.00\"> [0.74, 0.02, 0.24]</text>\n",
       "</g>\n",
       "<!-- Optimize weights \\n to minmize CE loss -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Optimize weights \\n to minmize CE loss</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"147.5,-129 18.5,-129 18.5,0 147.5,0 147.5,-129\"/>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">Optimize weights </text>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\"> to minmize CE loss</text>\n",
       "</g>\n",
       "<!-- NLL(Cross entropy loss) \\n \\n CE = &#45;∑_i(y_i * log(p_i)) \\n y_i = one hot label \\n p_i = class probabilities&#45;&gt;Optimize weights \\n to minmize CE loss -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>NLL(Cross entropy loss) \\n \\n CE = -∑_i(y_i * log(p_i)) \\n y_i = one hot label \\n p_i = class probabilities-&gt;Optimize weights \\n to minmize CE loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M83,-194.93C83,-176.59 83,-157.26 83,-139.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"86.5,-139.17 83,-129.17 79.5,-139.17 86.5,-139.17\"/>\n",
       "<text text-anchor=\"middle\" x=\"286\" y=\"-165.8\" font-family=\"Times,serif\" font-size=\"14.00\">Cross entropy loss </text>\n",
       "<text text-anchor=\"middle\" x=\"286\" y=\"-150.8\" font-family=\"Times,serif\" font-size=\"14.00\"> CE = -(1 * log(0.74) + 0 * log(0.02) + 0 * log(0.24)) = -log(0.74) = 0.13</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from io import StringIO\n",
    "from IPython.display import SVG\n",
    "import pydot\n",
    "\n",
    "dot_graph1 = pydot.Dot(graph_type='digraph')\n",
    "\n",
    "sd_node = pydot.Node('Standard NN')\n",
    "sd_node.set_shape('box3d')\n",
    "dot_graph1.add_node(sd_node)\n",
    "\n",
    "riq_node = pydot.Node('Softmax \\n (Cross entropy loss)')\n",
    "riq_node.set_shape('square')\n",
    "dot_graph1.add_node(riq_node)\n",
    "\n",
    "iedge = pydot.Edge(sd_node,riq_node)\n",
    "iedge.set_label('logits \\n [2, -1, 4]')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "ariq_node = pydot.Node('Predicted class = Max(logits)')\n",
    "ariq_node.set_shape('square')\n",
    "dot_graph1.add_node(ariq_node)\n",
    "\n",
    "aiedge = pydot.Edge(sd_node,ariq_node)\n",
    "aiedge.set_label('logits \\n [2, -1, 4]')\n",
    "dot_graph1.add_edge(aiedge)\n",
    "\n",
    "asp_node1 = pydot.Node('NLL(Cross entropy loss) \\n \\n CE = -∑_i(y_i * log(p_i)) \\n y_i = one hot label \\n p_i = class probabilities')\n",
    "asp_node1.set_shape('square')\n",
    "dot_graph1.add_node(asp_node1)\n",
    "\n",
    "iedge = pydot.Edge(riq_node, asp_node1)\n",
    "iedge.set_label('Class probabilities \\n [0.74, 0.02, 0.24]')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "asp_node2 = pydot.Node('Optimize weights \\n to minmize CE loss')\n",
    "asp_node2.set_shape('square')\n",
    "dot_graph1.add_node(asp_node2)\n",
    "\n",
    "iedge = pydot.Edge(asp_node1, asp_node2)\n",
    "iedge.set_label('Cross entropy loss \\n CE = -(1 * log(0.74) + 0 * log(0.02) + 0 * log(0.24)) = -log(0.74) = 0.13')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "\n",
    "dot_graph1.write_svg('big_data1.svg')\n",
    "dot_graph1.write_ps2('big_data1.ps2')\n",
    "SVG('big_data1.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649afc8b",
   "metadata": {},
   "source": [
    "* Multinomial distribution (discrete class probabilities) - Softmax is used in the last layer to convert the continuous activations of output layer to class probabilities\n",
    "* Neural Network is responsible for adjusting the ratio of class probabilities and softmax squashes the ratio to simplex\n",
    "* The softmax squashed multinomial likelihood is then maximised w.r.t $\\theta$\n",
    "* Cross entropy loss (NLL + logsoftmax) -log p(y|x,$\\theta$) = -log $\\sigma(f_{y}$|x,$\\theta$)\n",
    "* $\\sigma(u_{j})=\\frac{e^{uj}}{\\sum_{i=1}^{K}e^{uk}}$\n",
    "* MLE underestimates the true variance and the exponentiation in the softmax function can result in probabilities that are larger than 1 and therefore, may lead to the inflation of the probability of the predicted class. \n",
    "* If the softmax function inflates the predicted probabilities, then the distances between the predicted class and other classes may not accurately reflect the true uncertainty of the model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3e255",
   "metadata": {},
   "source": [
    "## Evidential Deep Learning\n",
    "The outputs of a deep learning model are treated as probability distributions over class labels. These distributions can be used to quantify the uncertainty of the model's predictions in a number of ways. <br>\n",
    "Example: The entropy of the probability distribution (measurement of the amount of uncertainty in the model's predictions)\n",
    "\n",
    "Softmax, the standard output of a classification network is interpreted as the parameter set of a categorical distribution. By replacing this parameter set with the parameters of a Dirichlet density, the predictions of the learner is represented as a distribution over possible softmax outputs, rather than the point estimate of a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c343d1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"339pt\" height=\"660pt\" viewBox=\"0.00 0.00 338.50 660.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 656)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-656 334.5,-656 334.5,4 -4,4\"/>\n",
       "<!-- Standard NN -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Standard NN</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"199.5,-652 115.5,-652 111.5,-648 111.5,-616 195.5,-616 199.5,-620 199.5,-652\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"195.5,-648 111.5,-648 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"195.5,-648 195.5,-616 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"195.5,-648 199.5,-652 \"/>\n",
       "<text text-anchor=\"middle\" x=\"155.5\" y=\"-630.3\" font-family=\"Times,serif\" font-size=\"14.00\">Standard NN</text>\n",
       "</g>\n",
       "<!-- Relu/Exponential \\n (activation function) -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Relu/Exponential \\n (activation function)</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"134,-526.5 3,-526.5 3,-395.5 134,-395.5 134,-526.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-464.8\" font-family=\"Times,serif\" font-size=\"14.00\">Relu/Exponential </text>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-449.8\" font-family=\"Times,serif\" font-size=\"14.00\"> (activation function)</text>\n",
       "</g>\n",
       "<!-- Standard NN&#45;&gt;Relu/Exponential \\n (activation function) -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Standard NN-&gt;Relu/Exponential \\n (activation function)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M133.13,-615.7C127.46,-610.48 121.77,-604.43 117.5,-598 105.1,-579.33 95.39,-557.13 88,-536.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"91.29,-534.94 84.76,-526.59 84.66,-537.19 91.29,-534.94\"/>\n",
       "<text text-anchor=\"middle\" x=\"144\" y=\"-586.8\" font-family=\"Times,serif\" font-size=\"14.00\">logits </text>\n",
       "<text text-anchor=\"middle\" x=\"144\" y=\"-571.8\" font-family=\"Times,serif\" font-size=\"14.00\"> [2, -1, 4]</text>\n",
       "</g>\n",
       "<!-- Predicted class = Max(logits) -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Predicted class = Max(logits)</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"330.5,-550 152.5,-550 152.5,-372 330.5,-372 330.5,-550\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.5\" y=\"-457.3\" font-family=\"Times,serif\" font-size=\"14.00\">Predicted class = Max(logits)</text>\n",
       "</g>\n",
       "<!-- Standard NN&#45;&gt;Predicted class = Max(logits) -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Standard NN-&gt;Predicted class = Max(logits)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M164.91,-615.98C168.01,-610.29 171.45,-603.89 174.5,-598 180.99,-585.46 187.7,-572.21 194.25,-559.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"197.43,-560.56 198.76,-550.05 191.17,-557.44 197.43,-560.56\"/>\n",
       "<text text-anchor=\"middle\" x=\"216\" y=\"-586.8\" font-family=\"Times,serif\" font-size=\"14.00\">logits </text>\n",
       "<text text-anchor=\"middle\" x=\"216\" y=\"-571.8\" font-family=\"Times,serif\" font-size=\"14.00\"> [2, -1, 4]</text>\n",
       "</g>\n",
       "<!-- DST \\n u + ∑ b_k = 1 \\n \\n b_k = e_k/S \\n u = K/S \\n S = ∑(e_i + 1) -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>DST \\n u + ∑ b_k = 1 \\n \\n b_k = e_k/S \\n u = K/S \\n S = ∑(e_i + 1)</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"120,-306 17,-306 17,-203 120,-203 120,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-288.3\" font-family=\"Times,serif\" font-size=\"14.00\">DST </text>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-273.3\" font-family=\"Times,serif\" font-size=\"14.00\"> u + ∑ b_k = 1 </text>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-258.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-243.3\" font-family=\"Times,serif\" font-size=\"14.00\"> b_k = e_k/S </text>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-228.3\" font-family=\"Times,serif\" font-size=\"14.00\"> u = K/S </text>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-213.3\" font-family=\"Times,serif\" font-size=\"14.00\"> S = ∑(e_i + 1)</text>\n",
       "</g>\n",
       "<!-- Relu/Exponential \\n (activation function)&#45;&gt;DST \\n u + ∑ b_k = 1 \\n \\n b_k = e_k/S \\n u = K/S \\n S = ∑(e_i + 1) -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Relu/Exponential \\n (activation function)-&gt;DST \\n u + ∑ b_k = 1 \\n \\n b_k = e_k/S \\n u = K/S \\n S = ∑(e_i + 1)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M68.5,-395.29C68.5,-369.99 68.5,-341.13 68.5,-316.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"72,-316.18 68.5,-306.18 65,-316.18 72,-316.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"118.5\" y=\"-342.8\" font-family=\"Times,serif\" font-size=\"14.00\">evidence(e_k) </text>\n",
       "<text text-anchor=\"middle\" x=\"118.5\" y=\"-327.8\" font-family=\"Times,serif\" font-size=\"14.00\"> [0.74, 0.02, 0.24]</text>\n",
       "</g>\n",
       "<!-- Dirichlet Distribution \\n α_k = e_k + 1 \\n \\n Expected class \\n probability \\n p_k = α_k/S -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Dirichlet Distribution \\n α_k = e_k + 1 \\n \\n Expected class \\n probability \\n p_k = α_k/S</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"137,-137 0,-137 0,0 137,0 137,-137\"/>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\">Dirichlet Distribution </text>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-87.3\" font-family=\"Times,serif\" font-size=\"14.00\"> α_k = e_k + 1 </text>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-72.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-57.3\" font-family=\"Times,serif\" font-size=\"14.00\"> Expected class </text>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-42.3\" font-family=\"Times,serif\" font-size=\"14.00\"> probability </text>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-27.3\" font-family=\"Times,serif\" font-size=\"14.00\"> p_k = α_k/S</text>\n",
       "</g>\n",
       "<!-- DST \\n u + ∑ b_k = 1 \\n \\n b_k = e_k/S \\n u = K/S \\n S = ∑(e_i + 1)&#45;&gt;Dirichlet Distribution \\n α_k = e_k + 1 \\n \\n Expected class \\n probability \\n p_k = α_k/S -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>DST \\n u + ∑ b_k = 1 \\n \\n b_k = e_k/S \\n u = K/S \\n S = ∑(e_i + 1)-&gt;Dirichlet Distribution \\n α_k = e_k + 1 \\n \\n Expected class \\n probability \\n p_k = α_k/S</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M68.5,-202.73C68.5,-185.6 68.5,-166.07 68.5,-147.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"72,-147.33 68.5,-137.33 65,-147.33 72,-147.33\"/>\n",
       "<text text-anchor=\"middle\" x=\"118.5\" y=\"-173.8\" font-family=\"Times,serif\" font-size=\"14.00\">evidence(e_k) </text>\n",
       "<text text-anchor=\"middle\" x=\"118.5\" y=\"-158.8\" font-family=\"Times,serif\" font-size=\"14.00\"> [0.74, 0.02, 0.24]</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from io import StringIO\n",
    "from IPython.display import SVG\n",
    "import pydot\n",
    "\n",
    "dot_graph1 = pydot.Dot(graph_type='digraph')\n",
    "\n",
    "sd_node = pydot.Node('Standard NN')\n",
    "sd_node.set_shape('box3d')\n",
    "dot_graph1.add_node(sd_node)\n",
    "\n",
    "riq_node = pydot.Node('Relu/Exponential \\n (activation function)')\n",
    "riq_node.set_shape('square')\n",
    "dot_graph1.add_node(riq_node)\n",
    "\n",
    "iedge = pydot.Edge(sd_node,riq_node)\n",
    "iedge.set_label('logits \\n [2, -1, 4]')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "ariq_node = pydot.Node('Predicted class = Max(logits)')\n",
    "ariq_node.set_shape('square')\n",
    "dot_graph1.add_node(ariq_node)\n",
    "\n",
    "aiedge = pydot.Edge(sd_node,ariq_node)\n",
    "aiedge.set_label('logits \\n [2, -1, 4]')\n",
    "dot_graph1.add_edge(aiedge)\n",
    "\n",
    "asp_node1 = pydot.Node('DST \\n u + ∑ b_k = 1 \\n \\n b_k = e_k/S \\n u = K/S \\n S = ∑(e_i + 1)')\n",
    "asp_node1.set_shape('square')\n",
    "dot_graph1.add_node(asp_node1)\n",
    "\n",
    "iedge = pydot.Edge(riq_node, asp_node1)\n",
    "iedge.set_label('evidence(e_k) \\n [0.74, 0.02, 0.24]')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "asp_node2 = pydot.Node('Dirichlet Distribution \\n α_k = e_k + 1 \\n \\n Expected class \\n probability \\n p_k = α_k/S')\n",
    "asp_node2.set_shape('square')\n",
    "dot_graph1.add_node(asp_node2)\n",
    "\n",
    "iedge = pydot.Edge(asp_node1, asp_node2)\n",
    "iedge.set_label('evidence(e_k) \\n [0.74, 0.02, 0.24]')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "\n",
    "dot_graph1.write_svg('big_data1.svg')\n",
    "dot_graph1.write_ps2('big_data1.ps2')\n",
    "SVG('big_data1.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b72db85",
   "metadata": {},
   "source": [
    "- e evidence\n",
    "- b belief\n",
    "- u uncertainty\n",
    "- K number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39639d7",
   "metadata": {},
   "source": [
    "* The Dempster–Shafer Theory of Evidence (DST) assigns belief mass $b_{k}$ to all classes and an overall uncertainty mass u\n",
    "* All belief mass and uncertainty mass are non negative and sum upto one\n",
    "* u + $\\sum\\limits_{k=1}^K$ $b_{k}$ = 1\n",
    "* $b_{k}$ = $\\frac{e_{k}}{S}$\n",
    "* u = $\\frac{K}{S}$\n",
    "* S = $\\sum\\limits_{i=1}^K$ $e_{i}$+1\n",
    "* Uncertainty is inversely proportional to the total evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867bcfc8",
   "metadata": {},
   "source": [
    "* In dirichlet distribution parameters $\\alpha_{k}$ = $e_{k}$ + 1\n",
    "* $b_{k}$ = $\\frac{\\alpha_{k}-1}{S}$\n",
    "* S = $\\sum\\limits_{i=1}^K$ $\\alpha_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ef55b",
   "metadata": {},
   "source": [
    "* Replace the softmax layer of standard NN with a ReLU layer to ascertain non negtaive values and obtain evidences  $e_{1}$, $e_{2}$, $e_{3}$,..... $e_{K}$ \n",
    "* Dirichlet distribution parametrized over evidence represents the density of each such probability assignment. Hence it models second-order probabilities and uncertainty\n",
    "* Dirichlet distribution D(p∣α) is a pdf over categorical distribution parameterized by [$\\alpha_{1}$, $\\alpha_{2}$, $\\alpha_{3}$,..... $\\alpha_{K}$] (possible to sample class probability)\n",
    "* D(p∣$\\alpha$) = \n",
    "* Expected class probability, $\\hat{p}_{k}$ = $\\frac{\\alpha_{k}}{S}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02dc12",
   "metadata": {},
   "source": [
    "### Type II Maximum Likelihood loss\n",
    "* D(p∣α) is a prior on the likelihood Multi(y∣p) and the negative log marginal likelihood is calculated by integrating out the class probabilities\n",
    "* Loss, L($\\theta$) = -log(Marginal likelihood)\n",
    "$$L(\\theta) = -log(\\int\\prod_{j=1}^{k}p_{ij}^{y_{ij}}\\frac{1}{B(\\alpha_{i})}\\prod_{j=1}^{k}p_{ij}^{\\alpha_{ij}-1}dp_{i})$$\n",
    "$$L(\\theta) = \\sum \\limits_{j=1}^{k}y_{ij}(log(S_{i})-log(\\alpha_{ij}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b232d1",
   "metadata": {},
   "source": [
    "### Bayes risk with cross entropy loss\n",
    "$$L(\\theta) = \\int[\\sum\\limits_{j=1}^{k}-{y_{ij}log(p_{ij})}]\\frac{1}{B(\\alpha_{i})}\\prod_{j=1}^{k}p_{ij}^{\\alpha_{ij}-1}dp_{i}$$\n",
    "$$L(\\theta) = \\sum \\limits_{j=1}^{k}y_{ij}(\\psi(S_{i})-\\psi(\\alpha_{ij}))$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\psi$ is digamma function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cdd440",
   "metadata": {},
   "source": [
    "### Bayes risk with squared loss\n",
    "$$L(\\theta) = \\int ||y_{i}-p_{i}||_{2}^{2}\\frac{1}{B(\\alpha_{i})}\\prod_{j=1}^{k}p_{ij}^{\\alpha_{ij}-1}dp_{i}$$\n",
    "$$L(\\theta) = \\sum \\limits_{j=1}^{k}E[y_{ij}^2 - 2y_{ij}p_{ij} + p_{ij}^2]$$\n",
    "$$L(\\theta) = \\sum \\limits_{j=1}^{k}(y_{ij}^2 - 2y_{ij}E[p_{ij}] + E[p_{ij}^2])$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* $L(\\theta) = \\sum \\limits_{j=1}^{k}(y_{ij} - E[p_{ij}])^2) + Var(p_{ij})$\n",
    "* $L(\\theta)$ = $\\sum\\limits_{j=1}^K$ $L_{ij}^{err}$ + $L_{ij}^{var}$  \n",
    "* The loss aims to achieve the joint goals of minimizing the prediction error and the variance of the Dirichlet experiment\n",
    "* Prioritizes data fit over variance estimation\n",
    "\n",
    "**Proposition 1** \\n\n",
    "Proposition 1: For any $\\alpha_{ij}$ $\\geq$ 1, the inequality $L_{ij}^{var}$ < $L_{ij}^{err}$ is satisfied.\\\n",
    "i.e. The last prioritizes data fit over variance estimation\n",
    "\n",
    "**Proposition 2**\n",
    "For a given sample i with the correct label j, $L_{i}^{err}$ decreses when new evidence is added to $\\alpha_{ij}$ and increases when evidence is removed from $\\alpha_{ij}$\\\n",
    "i.e. The loss has a tendency to fit to the data\n",
    "\n",
    "**Proposition 3**\n",
    "For a given sample i with the correct class label j, $L_{i}^{err}$ decreases when some evidence is removed from the biggest dirichilet parameter $\\alpha_{ij}$ such that $l \\neq$ j.\\\n",
    "i.e. The loss performs learned loss attenuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eca748c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
