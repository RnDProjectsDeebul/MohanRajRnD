{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.9/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (1.22.4)\n",
      "Requirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics) (4.3.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.1->torchmetrics) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.1->torchmetrics) (63.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.9)\n",
      "Requirement already satisfied: neptune-client in /opt/conda/lib/python3.9/site-packages (0.16.15)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.9/site-packages (from neptune-client) (0.18.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from neptune-client) (1.16.0)\n",
      "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /opt/conda/lib/python3.9/site-packages (from neptune-client) (3.0.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from neptune-client) (21.3)\n",
      "Requirement already satisfied: requests-oauthlib>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from neptune-client) (1.3.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from neptune-client) (1.4.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from neptune-client) (5.9.1)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.9/site-packages (from neptune-client) (2.28.1)\n",
      "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /opt/conda/lib/python3.9/site-packages (from neptune-client) (1.3.3)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.9/site-packages (from neptune-client) (1.26.11)\n",
      "Requirement already satisfied: Pillow>=1.1.6 in /opt/conda/lib/python3.9/site-packages (from neptune-client) (9.2.0)\n",
      "Requirement already satisfied: oauthlib>=2.1.0 in /opt/conda/lib/python3.9/site-packages (from neptune-client) (3.2.0)\n",
      "Requirement already satisfied: PyJWT in /opt/conda/lib/python3.9/site-packages (from neptune-client) (2.4.0)\n",
      "Requirement already satisfied: boto3>=1.16.0 in /opt/conda/lib/python3.9/site-packages (from neptune-client) (1.26.37)\n",
      "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in /opt/conda/lib/python3.9/site-packages (from neptune-client) (11.0.3)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from neptune-client) (8.1.3)\n",
      "Requirement already satisfied: GitPython>=2.0.8 in /opt/conda/lib/python3.9/site-packages (from neptune-client) (3.1.29)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3>=1.16.0->neptune-client) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from boto3>=1.16.0->neptune-client) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.37 in /opt/conda/lib/python3.9/site-packages (from boto3>=1.16.0->neptune-client) (1.29.37)\n",
      "Requirement already satisfied: msgpack in /opt/conda/lib/python3.9/site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.0.4)\n",
      "Requirement already satisfied: monotonic in /opt/conda/lib/python3.9/site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.6)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (6.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.9/site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (2.8.2)\n",
      "Requirement already satisfied: simplejson in /opt/conda/lib/python3.9/site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (4.3.0)\n",
      "Requirement already satisfied: bravado-core>=5.16.1 in /opt/conda/lib/python3.9/site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (5.17.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython>=2.0.8->neptune-client) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20.0->neptune-client) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20.0->neptune-client) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20.0->neptune-client) (3.3)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.9/site-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.9.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->neptune-client) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->neptune-client) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.9/site-packages (from pandas->neptune-client) (1.22.4)\n",
      "Requirement already satisfied: jsonref in /opt/conda/lib/python3.9/site-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.0.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (5.0.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (22.1.0)\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.9/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.2.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.9/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.12)\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.9/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.1)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.9/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2.3)\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.9/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (20.11.0)\n",
      "Requirement already satisfied: rfc3987 in /opt/conda/lib/python3.9/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.3.8)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.9/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.1.4)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.9/site-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.2.3)\n",
      "Requirement already satisfied: scikit-plot in /opt/conda/lib/python3.9/site-packages (0.3.7)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from scikit-plot) (3.5.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /opt/conda/lib/python3.9/site-packages (from scikit-plot) (1.1.2)\n",
      "Requirement already satisfied: joblib>=0.10 in /opt/conda/lib/python3.9/site-packages (from scikit-plot) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.9 in /opt/conda/lib/python3.9/site-packages (from scikit-plot) (1.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (4.34.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.22.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.18->scikit-plot) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n",
      "Requirement already satisfied: ray[tune] in /opt/conda/lib/python3.9/site-packages (2.2.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (8.1.3)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (1.3.3)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (1.3.1)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (4.9.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (1.22.4)\n",
      "Requirement already satisfied: virtualenv>=20.0.24 in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (20.17.1)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (3.20.1)\n",
      "Requirement already satisfied: grpcio>=1.32.0 in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (1.47.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (3.8.2)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (22.1.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (1.0.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (2.28.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (6.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (1.4.3)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (0.9.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (2.5.1)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.9/site-packages (from grpcio>=1.32.0->ray[tune]) (1.16.0)\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.0.24->ray[tune]) (2.6.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.0.24->ray[tune]) (0.3.6)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema->ray[tune]) (0.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->ray[tune]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->ray[tune]) (2022.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->ray[tune]) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->ray[tune]) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->ray[tune]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->ray[tune]) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n",
    "!pip install neptune-client\n",
    "!pip install scikit-plot\n",
    "!pip install -U \"ray[tune]\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import resnet18, mobilenet_v2, ResNet18_Weights\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "import seaborn as sn\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import scipy.ndimage as nd\n",
    "import neptune.new as neptune\n",
    "from sklearn.metrics import confusion_matrix ,classification_report,accuracy_score,f1_score,precision_score,recall_score\n",
    "from scikitplot.metrics import plot_confusion_matrix\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52779/4119295516.py:12: NeptuneDeprecationWarning: `init` is deprecated, use `init_run` instead. We'll end support of it in `neptune-client==1.0.0`.\n",
      "  nep_logger = neptune.init(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/mohan20325145/resnet/e/RESNETNEP-196\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "#General\n",
    "data_dir = os.path.abspath(\"./data\")\n",
    "classes  = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "class_len = len(classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "training_config  = 0\n",
    "train_accuracy = Accuracy(task=\"multiclass\", num_classes = class_len)\n",
    "train_accuracy.to(device)\n",
    "l1 = l2 = lr = batch_size = 0\n",
    "\n",
    "tnsr_board_logger = SummaryWriter()\n",
    "nep_logger = neptune.init(\n",
    "project=\"mohan20325145/resnet\",\n",
    "api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhZWQyMTU4OC02NmU4LTRiNjgtYWE5Zi1lNDg5MjdmZGJhNzYifQ==\",)\n",
    "\n",
    "\n",
    "#Tuning \n",
    "tune_hyperparams = False\n",
    "num_samples = 4\n",
    "max_num_epochs = 3\n",
    "gpus_per_trial = 0 \n",
    "\n",
    "\n",
    "#Training\n",
    "num_workers = 2\n",
    "epochs = [30]\n",
    "optimizer = [\"SGD\", \"Adam\"]\n",
    "criterion = [\"edl_mse\"]\n",
    "# criterion = [\"edl_mse\", \"edl_log\", \"edl_digamma\", \"cross_entropy\"]\n",
    "\n",
    "\n",
    "model = [\"ResNet\"]\n",
    "train_network = True\n",
    "test_network = False\n",
    "save_model_params = True\n",
    "\n",
    "#num_workers = [2,3]\n",
    "#criterion = [\"edl_mse\", \"edl_digamma\", \"edl_log\", \"cross_entropy\", nn.NLLLoss(), nn.GaussianNLLLoss(), nn.SoftMarginLoss()] \n",
    "#optimizer = [\"Adam\", \"SGD\", \"ASGD\", \"Adamax\"]\n",
    "#epochs = [40, 100]\n",
    "#model = [\"ResNet\", \"MobileNet\", \"CustomNet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.RandomErasing()])\n",
    "    trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)   \n",
    "    return trainset, testset\n",
    "\n",
    "def logger():\n",
    "    nep_logger.stop()\n",
    "    tnsr_board_logger.close\n",
    "    # !tensorboard --logdir=runs\n",
    "\n",
    "class Param_Tuning_NN(nn.Module):\n",
    "    def __init__(self, l1, l2):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)   \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x  \n",
    "    \n",
    "class Custom_Train_NN(nn.Module):\n",
    "    def __init__(self, l1, l2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_subroutine(config, checkpoint_dir=None, data_dir=None):\n",
    "    \n",
    "    net = Param_Tuning_NN(config[\"l1\"], config[\"l2\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        \n",
    "    trainset, testset = load_data(data_dir)\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(trainset, [test_abs, len(trainset) - test_abs])\n",
    "    trainloader = torch.utils.data.DataLoader(train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8)\n",
    "    valloader = torch.utils.data.DataLoader(val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
    "    \n",
    "    \n",
    "def tune_model():\n",
    "    config = {\"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "              \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "              \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "              \"batch_size\": tune.choice([2, 4, 8, 16])}\n",
    "    scheduler = ASHAScheduler(metric=\"loss\",\n",
    "                              mode=\"min\",\n",
    "                              max_t=max_num_epochs,\n",
    "                              grace_period=1,\n",
    "                              reduction_factor=2)\n",
    "    reporter = CLIReporter(metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(partial(tune_subroutine, data_dir=data_dir),\n",
    "             resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "             config=config,\n",
    "             num_samples=num_samples,\n",
    "             scheduler=scheduler,\n",
    "             progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n",
    "    return best_trial.config['l1'], best_trial.config['l2'], best_trial.config['lr'], best_trial.config['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_evidence(y):\n",
    "    return F.relu(y)\n",
    "\n",
    "\n",
    "def exp_evidence(y):\n",
    "    return torch.exp(torch.clamp(y, -5, 5))\n",
    "\n",
    "\n",
    "def softplus_evidence(y):\n",
    "    return F.softplus(y)\n",
    "\n",
    "\n",
    "def kl_divergence(alpha, num_classes, device=None):\n",
    "    ones = torch.ones([1, num_classes], dtype=torch.float32, device=device)\n",
    "    sum_alpha = torch.sum(alpha, dim=1, keepdim=True)\n",
    "    first_term = (torch.lgamma(sum_alpha)\n",
    "        - torch.lgamma(alpha).sum(dim=1, keepdim=True)\n",
    "        + torch.lgamma(ones).sum(dim=1, keepdim=True)\n",
    "        - torch.lgamma(ones.sum(dim=1, keepdim=True)))\n",
    "    second_term = ((alpha - ones)\n",
    "        .mul(torch.digamma(alpha) - torch.digamma(sum_alpha))\n",
    "        .sum(dim=1, keepdim=True))\n",
    "    kl = first_term + second_term\n",
    "    return kl\n",
    "\n",
    "\n",
    "def loglikelihood_loss(y, alpha, device=None):\n",
    "    y = y.to(device)\n",
    "    alpha = alpha.to(device)\n",
    "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
    "    loglikelihood_err = torch.sum((y - (alpha / S)) ** 2, dim=1, keepdim=True)\n",
    "    loglikelihood_var = torch.sum(alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True)\n",
    "    loglikelihood = loglikelihood_err + loglikelihood_var\n",
    "    return loglikelihood\n",
    "\n",
    "\n",
    "def mse_loss(y, alpha, epoch_num, num_classes, annealing_step, device=None):\n",
    "    y = y.to(device)\n",
    "    alpha = alpha.to(device)\n",
    "    loglikelihood = loglikelihood_loss(y, alpha, device=device)\n",
    "    annealing_coef = torch.min(torch.tensor(1.0, dtype=torch.float32),torch.tensor(epoch_num / annealing_step, dtype=torch.float32),)\n",
    "    kl_alpha = (alpha - 1) * (1 - y) + 1\n",
    "    kl_div = annealing_coef * kl_divergence(kl_alpha, num_classes, device=device)\n",
    "    return loglikelihood + kl_div\n",
    "\n",
    "\n",
    "def edl_loss(func, y, alpha, epoch_num, num_classes, annealing_step, device=None):\n",
    "    y = y.to(device)\n",
    "    alpha = alpha.to(device)\n",
    "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
    "    A = torch.sum(y * (func(S) - func(alpha)), dim=1, keepdim=True)\n",
    "    annealing_coef = torch.min(torch.tensor(1.0, dtype=torch.float32),torch.tensor(epoch_num / annealing_step, dtype=torch.float32),)\n",
    "    kl_alpha = (alpha - 1) * (1 - y) + 1\n",
    "    kl_div = annealing_coef * kl_divergence(kl_alpha, num_classes, device=device)\n",
    "    return A + kl_div\n",
    "\n",
    "\n",
    "def edl_mse_loss(output, target, epoch_num, num_classes, annealing_step, device):\n",
    "    evidence = relu_evidence(output)\n",
    "    alpha = evidence + 1\n",
    "    loss = torch.mean(mse_loss(target, alpha, epoch_num, num_classes, annealing_step, device))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def edl_log_loss(output, target, epoch_num, num_classes, annealing_step, device):\n",
    "    evidence = relu_evidence(output)\n",
    "    alpha = evidence + 1\n",
    "    loss = torch.mean(edl_loss(torch.log, target, alpha, epoch_num, num_classes, annealing_step, device))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def edl_digamma_loss(output, target, epoch_num, num_classes, annealing_step, device):\n",
    "    evidence = relu_evidence(output)\n",
    "    alpha = evidence + 1\n",
    "    loss = torch.mean(edl_loss(torch.digamma, target, alpha, epoch_num, num_classes, annealing_step, device))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_subroutine(criterion_, optimizer_, epochs_, model_):\n",
    "    since = time.time()\n",
    "    nep_logger['params/config'+ str(training_config)].log(model_)\n",
    "    nep_logger['params/config'+ str(training_config)].log(epochs_)\n",
    "    nep_logger['params/config'+ str(training_config)].log(optimizer_)\n",
    "    nep_logger['params/config'+ str(training_config)].log(criterion_)\n",
    "    \n",
    "    if (model_ == \"ResNet\"):\n",
    "        weights = ResNet18_Weights.DEFAULT\n",
    "        net = resnet18(weights=weights)\n",
    "        net.fc = nn.Linear(in_features=512,out_features=class_len)\n",
    "    elif (model_ == \"MobileNet\"):\n",
    "        net = mobilenet_v2()\n",
    "        net.fc = nn.Linear(in_features=512,out_features=class_len)\n",
    "    elif (model_ == \"CustomNet\"):\n",
    "        net = Custom_Train_NN(l1, l2)   \n",
    "\n",
    "    if (optimizer_ == \"Adam\"):\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=0.005)\n",
    "    elif (optimizer_ == \"SGD\"):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "   \n",
    "    net = net.to(device)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=20, shuffle=True, num_workers=num_workers)\n",
    "    dataloaders = {\"train\": trainloader, \"val\": testloader}\n",
    "    \n",
    "    \n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs_):\n",
    "        print(\"Epoch {}/{}\".format(epoch, epochs_ - 1))\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                net.train()  \n",
    "            else:\n",
    "                net.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    y = F.one_hot(labels, num_classes=class_len)\n",
    "                    y.to(device)\n",
    "                    if criterion_ == \"edl_mse\":\n",
    "                        loss = edl_mse_loss(outputs, y.float(), epoch, class_len, 3, device)\n",
    "                    elif criterion_ == \"edl_log\":\n",
    "                        loss = edl_log_loss(outputs, y.float(), epoch, class_len, 3, device)\n",
    "                    elif criterion_ == \"edl_digamma\":\n",
    "                        loss = edl_digamma_loss(outputs, y.float(), epoch, class_len, 3, device)\n",
    "                    elif criterion_ == \"cross_entropy\":\n",
    "                        loss_fn = nn.CrossEntropyLoss()\n",
    "                        loss = loss_fn(outputs, labels)\n",
    "                    else:\n",
    "                        loss = criterion_(outputs, labels)\n",
    "                        \n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += loss.item()* inputs.size(0)\n",
    "                running_corrects += torch.sum(predicted == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                # print (\"LR :\", scheduler.get_lr())\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                nep_logger[f\"plots/{str(training_config)}-train-accuracy-{criterion_}-{optimizer_}-{epochs_}-{model_}\"].log(epoch_loss)\n",
    "                nep_logger[f\"plots/{str(training_config)}-train-loss-{criterion_}-{optimizer_}-{epochs_}-{model_}\"].log(epoch_acc.item())\n",
    "                if epoch == epochs_ - 1:\n",
    "                    nep_logger['params/train_epoch_loss'+ str(training_config)].log(epoch_loss)\n",
    "                    nep_logger['params/train_epoch_accuracy'+ str(training_config)].log(epoch_acc.item()) \n",
    "            else:\n",
    "                nep_logger[f\"plots/{str(training_config)}-val-accuracy-{criterion_}-{optimizer_}-{epochs_}-{model_}\"].log(epoch_loss)\n",
    "                nep_logger[f\"plots/{str(training_config)}-val-loss-{criterion_}-{optimizer_}-{epochs_}-{model_}\"].log(epoch_acc.item())\n",
    "                if epoch == epochs_ - 1:\n",
    "                    nep_logger['params/val_epoch_loss'+ str(training_config)].log(epoch_loss)\n",
    "                    nep_logger['params/val_epoch_accuracy'+ str(training_config)].log(epoch_acc.item()) \n",
    "                \n",
    "            \n",
    "            print(\"{} loss: {:.4f} acc: {:.4f}\".format(phase.capitalize(), epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(net.state_dict())\n",
    "                \n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print(\"Training complete in {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"Best val Acc: {:4f}\".format(best_acc))\n",
    "    \n",
    "    net.load_state_dict(best_model_wts)\n",
    "    if save_model_params:\n",
    "        state = {\"epoch\": epochs_,\n",
    "                 \"model_state_dict\": net.state_dict(),\n",
    "                 \"optimizer_state_dict\": optimizer.state_dict()}\n",
    "        \n",
    "        if criterion_ == \"Evidential\":\n",
    "            saved_models_count = len('./results/EDL/*')\n",
    "            torch.save(state, \"./results/EDL/\"+ str(saved_models_count+1)+\".pt\")\n",
    "        else:\n",
    "            saved_models_count = len('./results/CEL/*')\n",
    "            torch.save(state, \"./results/CEL/\"+ str(saved_models_count+1)+\".pt\")\n",
    "    \n",
    "\n",
    "def train_model():\n",
    "    global training_config\n",
    "    for loss, opti, epo, mod in [(loss, opti, epo, mod) for loss in criterion for opti in optimizer for epo in epochs for mod in model]:\n",
    "        training_config += 1\n",
    "        train_model_subroutine(loss, opti, epo, mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    for loss, mod in [(loss, mod) for loss in criterion for mod in model]:\n",
    "        if loss == \"Evidential\":\n",
    "            saved_models_count = len('./results/EDL/*')\n",
    "            checkpoint = torch.load(\"./results/EDL/\"+str(saved_models_count)+\".pt\")\n",
    "            \n",
    "        else:\n",
    "            saved_models_count = len('./results/CEL/*')\n",
    "            checkpoint = torch.load(\"./results/CEL/\"+str(saved_models_count)+\".pt\")\n",
    "            \n",
    "        if (model_ == \"ResNet\"):\n",
    "            net = resnet18()\n",
    "            net.fc = nn.Linear(in_features=512,out_features=class_len)\n",
    "            \n",
    "        optimizer = optim.Adam(net.parameters())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 0/29\n",
      "----------\n",
      "Train loss: 0.7653 acc: 0.4336\n",
      "Val loss: 0.6023 acc: 0.5988\n",
      "Epoch 1/29\n",
      "----------\n",
      "Train loss: 0.8362 acc: 0.6172\n",
      "Val loss: 0.7434 acc: 0.6687\n",
      "Epoch 2/29\n",
      "----------\n",
      "Train loss: 0.8160 acc: 0.6438\n",
      "Val loss: 0.7661 acc: 0.6789\n",
      "Epoch 3/29\n",
      "----------\n",
      "Train loss: 0.8183 acc: 0.6400\n",
      "Val loss: 0.7908 acc: 0.6287\n",
      "Epoch 4/29\n",
      "----------\n",
      "Train loss: 0.7948 acc: 0.6414\n",
      "Val loss: 0.7769 acc: 0.6569\n",
      "Epoch 5/29\n",
      "----------\n",
      "Train loss: 0.7595 acc: 0.6602\n",
      "Val loss: 0.7521 acc: 0.6732\n",
      "Epoch 6/29\n",
      "----------\n",
      "Train loss: 0.7343 acc: 0.6743\n",
      "Val loss: 0.7436 acc: 0.6834\n",
      "Epoch 7/29\n",
      "----------\n",
      "Train loss: 0.7118 acc: 0.6839\n",
      "Val loss: 0.7344 acc: 0.6844\n",
      "Epoch 8/29\n",
      "----------\n",
      "Train loss: 0.7015 acc: 0.6881\n",
      "Val loss: 0.7169 acc: 0.6878\n",
      "Epoch 9/29\n",
      "----------\n",
      "Train loss: 0.6885 acc: 0.6945\n",
      "Val loss: 0.7293 acc: 0.6825\n",
      "Epoch 10/29\n",
      "----------\n",
      "Train loss: 0.6685 acc: 0.7054\n",
      "Val loss: 0.7079 acc: 0.7124\n",
      "Epoch 11/29\n",
      "----------\n",
      "Train loss: 0.6544 acc: 0.7149\n",
      "Val loss: 0.7111 acc: 0.7157\n",
      "Epoch 12/29\n",
      "----------\n",
      "Train loss: 0.6456 acc: 0.7200\n",
      "Val loss: 0.7076 acc: 0.7065\n",
      "Epoch 13/29\n",
      "----------\n",
      "Train loss: 0.6363 acc: 0.7228\n",
      "Val loss: 0.6988 acc: 0.7102\n",
      "Epoch 14/29\n",
      "----------\n",
      "Train loss: 0.6289 acc: 0.7279\n",
      "Val loss: 0.7065 acc: 0.7173\n",
      "Epoch 15/29\n",
      "----------\n",
      "Train loss: 0.6155 acc: 0.7325\n",
      "Val loss: 0.6949 acc: 0.7185\n",
      "Epoch 16/29\n",
      "----------\n",
      "Train loss: 0.6070 acc: 0.7353\n",
      "Val loss: 0.6894 acc: 0.7169\n",
      "Epoch 17/29\n",
      "----------\n",
      "Train loss: 0.5983 acc: 0.7399\n",
      "Val loss: 0.7008 acc: 0.7217\n",
      "Epoch 18/29\n",
      "----------\n",
      "Train loss: 0.6005 acc: 0.7395\n",
      "Val loss: 0.6940 acc: 0.7286\n",
      "Epoch 19/29\n",
      "----------\n",
      "Train loss: 0.5948 acc: 0.7425\n",
      "Val loss: 0.6981 acc: 0.7237\n",
      "Epoch 20/29\n",
      "----------\n",
      "Train loss: 0.5878 acc: 0.7466\n",
      "Val loss: 0.6941 acc: 0.7337\n",
      "Epoch 21/29\n",
      "----------\n",
      "Train loss: 0.5853 acc: 0.7477\n",
      "Val loss: 0.7005 acc: 0.7357\n",
      "Epoch 22/29\n",
      "----------\n",
      "Train loss: 0.5852 acc: 0.7470\n",
      "Val loss: 0.6854 acc: 0.7353\n",
      "Epoch 23/29\n",
      "----------\n",
      "Train loss: 0.5771 acc: 0.7529\n",
      "Val loss: 0.7004 acc: 0.7305\n",
      "Epoch 24/29\n",
      "----------\n",
      "Train loss: 0.5744 acc: 0.7531\n",
      "Val loss: 0.6718 acc: 0.7359\n",
      "Epoch 25/29\n",
      "----------\n",
      "Train loss: 0.5728 acc: 0.7535\n",
      "Val loss: 0.7001 acc: 0.7360\n",
      "Epoch 26/29\n",
      "----------\n",
      "Train loss: 0.5734 acc: 0.7528\n",
      "Val loss: 0.6862 acc: 0.7329\n",
      "Epoch 27/29\n",
      "----------\n",
      "Train loss: 0.5741 acc: 0.7522\n",
      "Val loss: 0.6872 acc: 0.7412\n",
      "Epoch 28/29\n",
      "----------\n",
      "Train loss: 0.5739 acc: 0.7531\n",
      "Val loss: 0.6908 acc: 0.7362\n",
      "Epoch 29/29\n",
      "----------\n",
      "Train loss: 0.5705 acc: 0.7566\n",
      "Val loss: 0.6766 acc: 0.7355\n",
      "Training complete in 6m 40s\n",
      "Best val Acc: 0.741200\n",
      "Epoch 0/29\n",
      "----------\n",
      "Train loss: 0.7271 acc: 0.4430\n",
      "Val loss: 0.7602 acc: 0.3978\n",
      "Epoch 1/29\n",
      "----------\n",
      "Train loss: 0.9849 acc: 0.1587\n",
      "Val loss: 0.9725 acc: 0.2094\n",
      "Epoch 2/29\n",
      "----------\n",
      "Train loss: 0.9772 acc: 0.2449\n",
      "Val loss: 0.9756 acc: 0.2813\n",
      "Epoch 3/29\n",
      "----------\n",
      "Train loss: 0.9790 acc: 0.2661\n",
      "Val loss: 0.9798 acc: 0.2352\n",
      "Epoch 4/29\n",
      "----------\n",
      "Train loss: 0.9795 acc: 0.2481\n",
      "Val loss: 0.9818 acc: 0.2373\n",
      "Epoch 5/29\n",
      "----------\n",
      "Train loss: 0.9791 acc: 0.2522\n",
      "Val loss: 0.9780 acc: 0.2558\n",
      "Epoch 6/29\n",
      "----------\n",
      "Train loss: 0.9787 acc: 0.2513\n",
      "Val loss: 0.9778 acc: 0.2612\n",
      "Epoch 7/29\n",
      "----------\n",
      "Train loss: 0.9782 acc: 0.2889\n",
      "Val loss: 0.9790 acc: 0.2731\n",
      "Epoch 8/29\n",
      "----------\n",
      "Train loss: 0.9778 acc: 0.3036\n",
      "Val loss: 0.9773 acc: 0.2760\n",
      "Epoch 9/29\n",
      "----------\n",
      "Train loss: 0.9777 acc: 0.3027\n",
      "Val loss: 0.9778 acc: 0.2826\n",
      "Epoch 10/29\n",
      "----------\n",
      "Train loss: 0.9765 acc: 0.3090\n",
      "Val loss: 0.9734 acc: 0.3253\n",
      "Epoch 11/29\n",
      "----------\n",
      "Train loss: 0.9761 acc: 0.3005\n",
      "Val loss: 0.9751 acc: 0.2851\n",
      "Epoch 12/29\n",
      "----------\n",
      "Train loss: 0.9757 acc: 0.2959\n",
      "Val loss: 0.9734 acc: 0.3055\n",
      "Epoch 13/29\n",
      "----------\n",
      "Train loss: 0.9753 acc: 0.2986\n",
      "Val loss: 0.9714 acc: 0.3132\n",
      "Epoch 14/29\n",
      "----------\n",
      "Train loss: 0.9751 acc: 0.2959\n",
      "Val loss: 0.9732 acc: 0.3085\n",
      "Epoch 15/29\n",
      "----------\n",
      "Train loss: 0.9734 acc: 0.3066\n",
      "Val loss: 0.9703 acc: 0.3123\n",
      "Epoch 16/29\n",
      "----------\n",
      "Train loss: 0.9723 acc: 0.2979\n",
      "Val loss: 0.9698 acc: 0.3057\n",
      "Epoch 17/29\n",
      "----------\n",
      "Train loss: 0.9720 acc: 0.2941\n",
      "Val loss: 0.9677 acc: 0.3189\n",
      "Epoch 18/29\n",
      "----------\n",
      "Train loss: 0.9720 acc: 0.2995\n",
      "Val loss: 0.9684 acc: 0.2938\n",
      "Epoch 19/29\n",
      "----------\n",
      "Train loss: 0.9708 acc: 0.3001\n",
      "Val loss: 0.9713 acc: 0.2875\n",
      "Epoch 20/29\n",
      "----------\n",
      "Train loss: 0.9689 acc: 0.3004\n",
      "Val loss: 0.9677 acc: 0.2991\n",
      "Epoch 21/29\n",
      "----------\n",
      "Train loss: 0.9681 acc: 0.3026\n",
      "Val loss: 0.9652 acc: 0.3018\n",
      "Epoch 22/29\n",
      "----------\n",
      "Train loss: 0.9674 acc: 0.3029\n",
      "Val loss: 0.9659 acc: 0.3063\n",
      "Epoch 23/29\n",
      "----------\n",
      "Train loss: 0.9670 acc: 0.3017\n",
      "Val loss: 0.9656 acc: 0.3046\n",
      "Epoch 24/29\n",
      "----------\n",
      "Train loss: 0.9669 acc: 0.2987\n",
      "Val loss: 0.9626 acc: 0.3119\n",
      "Epoch 25/29\n",
      "----------\n",
      "Train loss: 0.9653 acc: 0.3029\n",
      "Val loss: 0.9638 acc: 0.3064\n",
      "Epoch 26/29\n",
      "----------\n",
      "Train loss: 0.9644 acc: 0.3016\n",
      "Val loss: 0.9623 acc: 0.3027\n",
      "Epoch 27/29\n",
      "----------\n",
      "Train loss: 0.9639 acc: 0.3020\n",
      "Val loss: 0.9627 acc: 0.3024\n",
      "Epoch 28/29\n",
      "----------\n",
      "Train loss: 0.9640 acc: 0.2994\n",
      "Val loss: 0.9628 acc: 0.3033\n",
      "Epoch 29/29\n",
      "----------\n",
      "Train loss: 0.9625 acc: 0.3033\n",
      "Val loss: 0.9603 acc: 0.3063\n",
      "Training complete in 8m 43s\n",
      "Best val Acc: 0.397800\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/mohan20325145/resnet/e/RESNETNEP-196\n"
     ]
    }
   ],
   "source": [
    "#Data loader\n",
    "trainset, testset = load_data(data_dir)\n",
    "\n",
    "#Tune model params\n",
    "if tune_hyperparams == True:\n",
    "    l1, l2, lr, batch_size = tune_model() \n",
    "else:\n",
    "    l1 = 16\n",
    "    l2 = 8\n",
    "    lr = 0.001\n",
    "    batch_size = 40\n",
    "\n",
    "#Train model\n",
    "if train_network == True:\n",
    "    train_model()\n",
    "\n",
    "#Test model\n",
    "if test_network == True:\n",
    "    test_model()\n",
    "\n",
    "#Logging\n",
    "logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         train_accuracy.update(predicted, labels)\n",
    "#         epoch_accuracy = train_accuracy.compute()\n",
    "#         train_accuracy.reset() \n",
    "#         predicted = predicted.cpu().detach().numpy()\n",
    "#         labels = labels.cpu().detach().numpy()\n",
    "#         epoch_accuracy_score = accuracy_score(labels, predicted)\n",
    "#         epoch_precision_score = precision_score(labels, predicted, average='weighted')\n",
    "#         epoch_f1_score = f1_score(labels, predicted, average='weighted')\n",
    "#         epoch_recall_score = recall_score(labels, predicted, average='weighted')\n",
    "#         nep_logger['plots/training/accuracy_sklearn'+ str(training_config)].log(epoch_accuracy_score)\n",
    "#         nep_logger['plots/training/precision_score'+ str(training_config)].log(epoch_precision_score)\n",
    "#         nep_logger['plots/training/f1_score'+ str(training_config)].log(epoch_f1_score)\n",
    "#         nep_logger['plots/training/recall_score'+ str(training_config)].log(epoch_recall_score)\n",
    "#         tnsr_board_logger.add_scalar('plots/training/loss'+ str(training_config), epoch_loss, epoch)\n",
    "#             fig, axis = plt.subplots(figsize = (20,20))\n",
    "#             plot_confusion_matrix(labels, predicted, ax=axis, normalize=True)\n",
    "#             ticks = np.arange(class_len)\n",
    "#             plt.xticks(ticks, classes, rotation=0, fontsize=14)\n",
    "#             plt.yticks(ticks, classes, fontsize=14)\n",
    "#             plt.title('Confusion Matrix', fontsize=20)\n",
    "#             nep_logger[\"images/training/conf_matrix\"].upload(fig)       \n",
    "#         PATH = './cifar_net_' + str(training_count) + '.pth'\n",
    "#         torch.save(net.state_dict(), PATH)\n",
    "    \n",
    "    \n",
    "#                 match = torch.reshape(torch.eq(predicted, labels).float(), (-1, 1))\n",
    "#                 acc = torch.mean(match)\n",
    "#                 evidence = relu_evidence(outputs)    \n",
    "#                 alpha = evidence + 1\n",
    "#                 u = class_len / torch.sum(alpha, dim=1, keepdim=True)\n",
    "#                 total_evidence = torch.sum(evidence, 1, keepdim=True)\n",
    "#                 mean_evidence = torch.mean(total_evidence)\n",
    "#                 mean_evidence_succ = torch.sum(torch.sum(evidence, 1, keepdim=True) * match) / torch.sum(match + 1e-20)\n",
    "#                 mean_evidence_fail = torch.sum(torch.sum(evidence, 1, keepdim=True) * (1 - match)) / (torch.sum(torch.abs(1 - match)) + 1e-20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
