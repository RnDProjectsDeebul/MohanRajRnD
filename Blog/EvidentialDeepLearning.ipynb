{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ff8b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\r\n",
      "#\r\n",
      "base                     /Users/mohan/opt/anaconda3\r\n",
      "rnd                   *  /Users/mohan/opt/anaconda3/envs/rnd\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a0668",
   "metadata": {},
   "source": [
    "Evidential Deep Learning to Quantify Classification Uncertainty <br>\n",
    "Murat Sensoy, Lance Kaplan, Melih Kandemir <br>\n",
    "https://arxiv.org/abs/1806.01768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19839ff5",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Deep Learning models lack transparency in decision-making process. This leads to concerns about robustness and reliability in safety critical applications. A new approach called evidential deep learning has emerged to quantify uncertainty of a deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6b43fd",
   "metadata": {},
   "source": [
    "## NN with cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7536527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from io import StringIO\n",
    "from IPython.display import SVG\n",
    "import pydot\n",
    "\n",
    "dot_graph1 = pydot.Dot(graph_type='digraph')\n",
    "\n",
    "sd_node = pydot.Node('Standard NN')\n",
    "sd_node.set_shape('box3d')\n",
    "dot_graph1.add_node(sd_node)\n",
    "\n",
    "riq_node = pydot.Node('Softmax \\n (Cross entropy loss)')\n",
    "riq_node.set_shape('square')\n",
    "dot_graph1.add_node(riq_node)\n",
    "\n",
    "iedge = pydot.Edge(sd_node,riq_node)\n",
    "iedge.set_label('logits \\n [2, -1, 4]')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "ariq_node = pydot.Node('Predicted class = Max(logits)')\n",
    "ariq_node.set_shape('square')\n",
    "dot_graph1.add_node(ariq_node)\n",
    "\n",
    "aiedge = pydot.Edge(sd_node,ariq_node)\n",
    "aiedge.set_label('logits \\n [2, -1, 4]')\n",
    "dot_graph1.add_edge(aiedge)\n",
    "\n",
    "asp_node1 = pydot.Node('NLL(Cross entropy loss) \\n \\n CE = -∑_i(y_i * log(p_i)) \\n y_i = one hot label \\n p_i = class probabilities')\n",
    "asp_node1.set_shape('square')\n",
    "dot_graph1.add_node(asp_node1)\n",
    "\n",
    "iedge = pydot.Edge(riq_node, asp_node1)\n",
    "iedge.set_label('Class probabilities \\n [0.74, 0.02, 0.24]')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "asp_node2 = pydot.Node('Optimize weights \\n to minmize CE loss')\n",
    "asp_node2.set_shape('square')\n",
    "dot_graph1.add_node(asp_node2)\n",
    "\n",
    "iedge = pydot.Edge(asp_node1, asp_node2)\n",
    "iedge.set_label('Cross entropy loss \\n CE = -(1 * log(0.74) + 0 * log(0.02) + 0 * log(0.24)) = -log(0.74) = 0.13')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "\n",
    "# dot_graph1.write_svg('big_data1.svg')\n",
    "# dot_graph1.write_ps2('big_data1.ps2')\n",
    "# SVG('big_data1.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649afc8b",
   "metadata": {},
   "source": [
    "* Multinomial distribution (discrete class probabilities) - Softmax is used in the last layer to convert the continuous activations of output layer to class probabilities\n",
    "* Neural Network is responsible for adjusting the ratio of class probabilities and softmax squashes the ratio to simplex\n",
    "* The softmax squashed multinomial likelihood is then maximised w.r.t $\\theta$\n",
    "* Cross entropy loss (NLL + logsoftmax) -log p(y|x,$\\theta$) = -log $\\sigma(f_{y}$|x,$\\theta$)\n",
    "* $\\sigma(u_{j})=\\frac{e^{uj}}{\\sum_{i=1}^{K}e^{uk}}$\n",
    "* MLE underestimates the true variance and the exponentiation in the softmax function can result in probabilities that are larger than 1 and therefore, may lead to the inflation of the probability of the predicted class. \n",
    "* If the softmax function inflates the predicted probabilities, then the distances between the predicted class and other classes may not accurately reflect the true uncertainty of the model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e440f0c9",
   "metadata": {},
   "source": [
    "## Evidential Deep Learning\n",
    "The outputs of a deep learning model are treated as probability distributions over class labels. These distributions can be used to quantify the uncertainty of the model's predictions in a number of ways. <br>\n",
    "Example: The entropy of the probability distribution (measurement of the amount of uncertainty in the model's predictions)\n",
    "\n",
    "Softmax, the standard output of a classification network is interpreted as the parameter set of a categorical distribution. By replacing this parameter set with the parameters of a Dirichlet density, the predictions of the learner is represented as a distribution over possible softmax outputs, rather than the point estimate of a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526b57ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from io import StringIO\n",
    "from IPython.display import SVG\n",
    "import pydot\n",
    "\n",
    "dot_graph1 = pydot.Dot(graph_type='digraph')\n",
    "\n",
    "sd_node = pydot.Node('Standard NN')\n",
    "sd_node.set_shape('box3d')\n",
    "dot_graph1.add_node(sd_node)\n",
    "\n",
    "riq_node = pydot.Node('Relu/Exponential \\n (activation function)')\n",
    "riq_node.set_shape('square')\n",
    "dot_graph1.add_node(riq_node)\n",
    "\n",
    "iedge = pydot.Edge(sd_node,riq_node)\n",
    "iedge.set_label('logits \\n [2, -1, 4]')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "ariq_node = pydot.Node('Predicted class = Max(logits)')\n",
    "ariq_node.set_shape('square')\n",
    "dot_graph1.add_node(ariq_node)\n",
    "\n",
    "aiedge = pydot.Edge(sd_node,ariq_node)\n",
    "aiedge.set_label('logits \\n [2, -1, 4]')\n",
    "dot_graph1.add_edge(aiedge)\n",
    "\n",
    "asp_node1 = pydot.Node('DST \\n u + ∑ b_k = 1 \\n \\n b_k = e_k/S \\n u = K/S \\n S = ∑(e_i + 1)')\n",
    "asp_node1.set_shape('square')\n",
    "dot_graph1.add_node(asp_node1)\n",
    "\n",
    "iedge = pydot.Edge(riq_node, asp_node1)\n",
    "iedge.set_label('evidence(e_k) \\n [0.74, 0.02, 0.24]')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "asp_node2 = pydot.Node('Dirichlet Distribution \\n α_k = e_k + 1 \\n \\n Expected class \\n probability \\n p_k = α_k/S')\n",
    "asp_node2.set_shape('square')\n",
    "dot_graph1.add_node(asp_node2)\n",
    "\n",
    "iedge = pydot.Edge(asp_node1, asp_node2)\n",
    "iedge.set_label('evidence(e_k) \\n [0.74, 0.02, 0.24]')\n",
    "dot_graph1.add_edge(iedge)\n",
    "\n",
    "\n",
    "dot_graph1.write_svg('big_data1.svg')\n",
    "dot_graph1.write_ps2('big_data1.ps2')\n",
    "SVG('big_data1.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d73bb",
   "metadata": {},
   "source": [
    "- e evidence\n",
    "- b belief\n",
    "- u uncertainty\n",
    "- K number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b71fc1",
   "metadata": {},
   "source": [
    "* The Dempster–Shafer Theory of Evidence (DST) assigns belief mass $b_{k}$ to all classes and an overall uncertainty mass u\n",
    "* All belief mass and uncertainty mass are non negative and sum upto one\n",
    "* u + $\\sum\\limits_{k=1}^K$ $b_{k}$ = 1\n",
    "* $b_{k}$ = $\\frac{e_{k}}{S}$\n",
    "* u = $\\frac{K}{S}$\n",
    "* S = $\\sum\\limits_{i=1}^K$ $e_{i}$+1\n",
    "* Uncertainty is inversely proportional to the total evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9501a8",
   "metadata": {},
   "source": [
    "* In dirichlet distribution parameters $\\alpha_{k}$ = $e_{k}$ + 1\n",
    "* $b_{k}$ = $\\frac{\\alpha_{k}-1}{S}$\n",
    "* S = $\\sum\\limits_{i=1}^K$ $\\alpha_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc33e5",
   "metadata": {},
   "source": [
    "* Replace the softmax layer of standard NN with a ReLU layer to ascertain non negtaive values and obtain evidences  $e_{1}$, $e_{2}$, $e_{3}$,..... $e_{K}$ \n",
    "* Dirichlet distribution parametrized over evidence represents the density of each such probability assignment. Hence it models second-order probabilities and uncertainty\n",
    "* Dirichlet distribution D(p∣α) is a pdf over categorical distribution parameterized by [$\\alpha_{1}$, $\\alpha_{2}$, $\\alpha_{3}$,..... $\\alpha_{K}$] (possible to sample class probability)\n",
    "* D(p∣$\\alpha$) = \n",
    "* Expected class probability, $\\hat{p}_{k}$ = $\\frac{\\alpha_{k}}{S}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af3757b",
   "metadata": {},
   "source": [
    "### Type II Maximum Likelihood loss\n",
    "* D(p∣α) is a prior on the likelihood Multi(y∣p) and the negative log marginal likelihood is calculated by integrating out the class probabilities\n",
    "* Loss, L($\\theta$) = -log(Marginal likelihood)\n",
    "$$L(\\theta) = -log(\\int\\prod_{j=1}^{k}p_{ij}^{y_{ij}}\\frac{1}{B(\\alpha_{i})}\\prod_{j=1}^{k}p_{ij}^{\\alpha_{ij}-1}dp_{i})$$\n",
    "$$L(\\theta) = \\sum \\limits_{j=1}^{k}y_{ij}(log(S_{i})-log(\\alpha_{ij}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bbbaf6",
   "metadata": {},
   "source": [
    "### Bayes risk with cross entropy loss\n",
    "$$L(\\theta) = \\int[\\sum\\limits_{j=1}^{k}-{y_{ij}log(p_{ij})}]\\frac{1}{B(\\alpha_{i})}\\prod_{j=1}^{k}p_{ij}^{\\alpha_{ij}-1}dp_{i}$$\n",
    "$$L(\\theta) = \\sum \\limits_{j=1}^{k}y_{ij}(\\psi(S_{i})-\\psi(\\alpha_{ij}))$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\psi$ is digamma function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c195c",
   "metadata": {},
   "source": [
    "### Bayes risk with squared loss\n",
    "$$L_{i}(\\theta) = \\int ||y_{i}-p_{i}||_{2}^{2}\\frac{1}{B(\\alpha_{i})}\\prod_{j=1}^{k}p_{ij}^{\\alpha_{ij}-1}dp_{i}$$\n",
    "$$L_{i}(\\theta) = \\sum \\limits_{j=1}^{k}E[y_{ij}^2 - 2y_{ij}p_{ij} + p_{ij}^2]$$\n",
    "$$L_{i}(\\theta) = \\sum \\limits_{j=1}^{k}(y_{ij}^2 - 2y_{ij}E[p_{ij}] + E[p_{ij}^2])$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* $L_{i}(\\theta) = \\sum \\limits_{j=1}^{k}(y_{ij} - E[p_{ij}])^2) + Var(p_{ij})$\n",
    "* $L_{i}(\\theta)$ = $\\sum\\limits_{j=1}^K$ $L_{ij}^{err}$ + $L_{ij}^{var}$  \n",
    "* The loss aims to achieve the joint goals of minimizing the prediction error and the variance of the Dirichlet experiment\n",
    "* Prioritizes data fit over variance estimation\n",
    "\n",
    "**Proposition 1** <br>\n",
    "For any $\\alpha_{ij}$ $\\geq$ 1, the inequality $L_{ij}^{var}$ < $L_{ij}^{err}$ is satisfied.\\\n",
    "i.e. The loss prioritizes data fit over variance estimation\n",
    "\n",
    "**Proposition 2** <br>\n",
    "For a given sample i with the correct label j, $L_{i}^{err}$ decreses when new evidence is added to $\\alpha_{ij}$ and increases when evidence is removed from $\\alpha_{ij}$\\\n",
    "i.e. The loss has a tendency to fit to the data\n",
    "\n",
    "**Proposition 3** <br>\n",
    "For a given sample i with the correct class label j, $L_{i}^{err}$ decreases when some evidence is removed from the biggest dirichilet parameter $\\alpha_{ij}$ such that $l \\neq$ j.\\\n",
    "i.e. The loss performs learned loss attenuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c43ec75",
   "metadata": {},
   "source": [
    "### KL Divergence\n",
    "* Shrinking the total evidence to zero for a sample, if it cannot be correctly classified is preferred. This is achieved by incorporating Kullback-Leibler (KL) Divergence term in the loss function\n",
    "* Remove non-misleading evidence from predicted parameters $\\alpha$\n",
    "* The annealing coefficient $\\lambda_{t}$ is manipulated to gradullay increase the effect of KL divergence in the loss function \n",
    "$$L(\\theta) = \\sum \\limits_{i=1}^{N} L_{i}(\\theta) + \\lambda_{t}\\sum \\limits_{i=1}^{N}KL[D(p_{i}|\\hat{\\alpha_{i}})||D(p_{i}|<1,...,1>)]$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
